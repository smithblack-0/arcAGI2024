# Block Multimodal Encoding

## What is it?

Block Multimodal Encoding is an encoding mechanism designed to represent
data across different modes within an encoding or decoding transformer-like
context. This approach allows a single model to handle sequence-based
multimodal data, accommodating various data types such as text, voice, or
images.

It has been designed in response to the ARC-AGI challenge, in order to allow
the usage of a pretrained transformer with the challenge.

## How does it work?

Suppose we have a sequence of $N$ different multimodal data pieces we
wish to encode together. For example, consider a sequence consisting of text,
followed by an image, and then more text. This forms a series of blocks: a
Text block, an Image block, and another Text block.

Data is encoded or decoded in large contiguous segments called "Mode Blocks."
Each Mode Block consists of a Header section and a Payload section, each
containing a series of embeddings. The Header section includes a Mode
embedding, a Shape embedding, and a Sequence embedding. Each Block has a
tensor shape like:

$[\text{mode\_header}, \text{shape\_header}, \text{sequence\_header}, \text{payload} + \text{context}\ldots]$

### Headers

The headers are the three embeddings at the beginning of a given Mode Block:
Mode, Shape, and Sequence.

- **Mode Header**: An embedding that specifies the mode of operation (e.g.,
  text, image). There are $N+1$ integers associated with modes,
  representing different content types and an END mode, analogous to an EOS
  token.

- **Shape Header**: Each mode has specific Shape tensor predictors and
  embedding systems. A shape tensor, such as $[100]$ or $[10, 10]$,
  indicates the structure of the data, like a string of 100 tokens or a 10x10
  image. This tensor helps determine the payload length, calculated as the
  product of its dimensions.

- **Sequence Header**: An embedding indicating the block's position within
  the sequence. The first block is generated with a sequence tensor set to 0,
  the second to 1, and so forth, distinguishing between blocks.

These header embeddings are generated by the model as targets, similar to
tokens, and define multidimensional data with dynamic shapes.

### Payload

The payload is the actual content that needs to be read or generated. Each
mode has its own payload embedding mechanism, loss function, and prediction
strategy during decoding. In training, a payload starts as raw data, which is
then embedded and used as a prediction target for the next sequence.

The payload itself lacks positional information, which is provided by the
context.

### Context

The context provides additional information, such as position and mode,
injected into each generated payload feature. It consists of:

- **Header Context**: Derived by summing the three headers, this context is
  added to each payload tensor, ensuring the model understands the mode,
  shape, and sequence details.

- **Positional Context**: Created using the Shape tensor, this context
  consists of flattened multidimensional embeddings. These embeddings are
  generated on-the-fly during decoding to inject positional context into
  produced tensors, enabling the model to infer position-based insights.

For instance, a 100x100 image could have 100x100 multidimensional positional
encodings, which are then flattened into a singular embedding of shape 10,000.
## Data and Embeddings

In the Block Multimodal Encoding system, we process a sequence of multimodal
data into Block Tensors, which are then used to generate targets for training
and teacher-forcing embeddings. This setup ensures efficient processing and
consistency across different modes.

### Multimodal Sequence

The sequence consists of $N$ different data pieces, each potentially in a
different mode, such as text, image, or audio. For each piece of multimodal
information, the model extracts necessary features to convert the data into
processable tensors.

### Block Tensors

From each multimodal data piece, we derive four Block Tensors to guide the
model's understanding and processing:

1. **Mode Tensor**
   - **Purpose**: Identifies the mode (e.g., text, image) of the data piece.
   - **Shape**: $(B, 1)$, where $B$ is the batch size.
   - **Structure**: An integer representing the mode type, associated with a
     specific embedding.

2. **Shape Tensor**
   - **Purpose**: Describes the dimensions of the data piece, informing the
     model of its structure.
   - **Shape**: $(B, D)$, where $D$ is the number of dimensions.
   - **Structure**: A tensor of integers indicating the shape (e.g.,
     dimensions of an image or length of a text sequence).

3. **Sequence Tensor**
   - **Purpose**: Specifies the position of the data piece within the
     multimodal sequence.
   - **Shape**: $(B, 1)$
   - **Structure**: An integer indicating the order of the block in the
     sequence.

4. **Loss Select Tensor**
   - **Purpose**: Guides the loss calculation by specifying the mode of loss
     based on the current operation.
   - **Shape**: $(B, 1)$
   - **Structure**: An integer tensor specifying the loss condition for each
     entry. This helps the model differentiate between modes during loss
     calculation.

### Target Generation

For each mode, we produce targets used for loss calculation. The targets are
generated to efficiently use shared logits, minimizing the need to store
multiple distributions during training.

- **Logit Structure**: The output logit consists of $K$ entries, where each
  mode can assign these for different purposes. For instance, a mode might use
  all $K$ for vocabulary (text generation) or allocate $K = L + N$, where $L$
  and $N$ are dimensions for specific functions.

- **Targets**: The targets are $N$-dimensional, specifying the intended output
  for each region. Modes must define how they use the combined logit space,
  and any unused targets are set to a negative value to indicate
  inactivity.

- **Contract for Modes**: Each mode must specify how it uses the shared logit
  space, detailing how $K$ entries are allocated and how to handle unused
  targets.

### Teacher-Forcing Embeddings

During training, embeddings for each Mode Block are generated with
teacher-forcing, involving the following steps:

1. **Embedding the Mode**: Transform the mode integer into a learned embedding
   vector.

2. **Embedding the Shape**: Convert the shape tensor into an embedding that
   captures the dimensions and layout of the data piece.

3. **Embedding the Sequence**: Translate the sequence integer into an
   embedding that reflects its position in the multimodal sequence.

4. **Embedding the Payload**: Each payload entry is transformed into an
   embedding. For images, this might involve tokenizing pixels; for text, this
   could mean tokenizing words or characters.

These embeddings are produced initially without context injection. Context
injection occurs in later processing stages, allowing the model to focus on
the inherent properties of the data first.

### Loss Modes and Calculation

The model operates with multiple loss modes, facilitated by the Loss Select
Tensor. This tensor guides the model in determining which loss function to
apply based on the operation being performed:

- **Mode Loss**: Applied when generating a mode.
- **Shape Loss**: Each of the $N$ non-end modes requires shape generation
  conditions.
- **Payload Loss**: Each of the $N$ modes has a specific payload processing
  condition.
- **Padding Loss**: No loss is applied when in padding mode.

These modes are encoded using an integer tensor specifying one of the above
conditions. If calculated correctly, there are $2 + 2N$ states to handle. The
loss function uses this information to evaluate the appropriate loss.

### Process Summary

- **Input Sequence**: $N$ multimodal data pieces, each with a unique mode.
- **Transformation**: Extract Mode, Shape, Sequence, and Loss Select tensors.
- **Target Generation**: Create $N$-dimensional targets using shared logits.
- **Embedding Generation**: Produce teacher-forced embeddings for the entire
  sequence.

This structured approach ensures each data piece is consistently transformed
into tensors, facilitating efficient model training and adaptability across
various modes.
## The Generative Process

The Block Multimodal Encoding system uses a cyclical process to generate data,
consisting of four key steps. This cycle is repeated during both training and
evaluation.

### Steps in the Generative Process

1. **Mode Select**: Determine the mode for the next block. The model projects
   the current state into a probability distribution over possible modes, and
   the selected mode's embedding is integrated into the output stream.

2. **Shape Select**: Predict or select the shape of the output for the chosen
   mode. This step generates a shape tensor and its corresponding embedding to
   define the payload's structure. It concludes with the model knowing the
   shape of the output space.

3. **Sequence Synthesis**: Establish the block's position within the sequence
   and generate a sequence embedding to maintain order and provide context for
   subsequent steps.

4. **Payload Processing**: Generate or process the payload data. Compute
   positional encodings based on the block's location and shape, injecting
   context as needed. There must be an established mode, shape, and sequence
   before the context for payload processing can be defined.

### Logits and Probability Distributions

The model operates with a shared logit space, meaning a single feedforward
network produces a unified tensor of logits for all modes. This shared logit
space ensures that all distributions, despite differing contexts, coexist
within the same tensor structure.

- **Mode Logits**: Indicate the likelihood of each mode. The model uses these
  logits to decide the mode to generate next, based on the current state.

- **Shape Logits**: Represent probabilities for each dimension of the shape.
  These logits help predict or select the shape of the output space for the
  current mode.

- **Sequence Logits**: Although generated as part of the process, sequence
  logits do not influence the mode or shape selection. The sequence position
  is teacher-forced to correspond to the block number, ensuring consistent
  ordering without relying on a distribution.

- **Payload Logits**: Guide the generation or interpretation of each token or
  unit within the payload. They play a critical role in ensuring that the
  content generated or processed is aligned with the expected output.

### Training Process

During training, the model:

- Stores distributions for loss calculation.
- Uses targets specific to modes, shapes, and sequences to guide selection
  through teacher-forcing.
- Applies context through the sum of mode, shape, and sequence headers.

### Evaluation Process

During evaluation, the model:

- Samples from the probability distributions to make decisions at each step.
- Relies on the generated distributions to autonomously determine the mode,
  shape, and payload processing.

### Process Overview

This generative process efficiently manages data generation by using logits to
make informed decisions and injecting context to ensure consistency across
various modes and structures. The cycle repeats, continually selecting new
modes and processing subsequent blocks.

## Losses and Distributions

We use a 

TODO: 

* Talk about calculating cross entropy with a multidistribution logit.
* Talk about sampling with a multidistribution logit.
# Block Multimodel Decoding: Detailed Plan

## 1. Overview
- **Objective**: To develop a block-based multimodel decoding system capable of processing, generating, and inferring across multiple modes of data (e.g., text, images, audio) using a generative modeling approach.
- **Core Components**:
  - **Block Structure**: Data is divided into blocks of fixed shape, allowing the generation of content of varying sizes.
  - **Multiple Models**: Specialized models are used for different data modes, all integrated through a shared common context.
  - **Master Model**: Predicts the next mode and block size to generate, guiding the overall generation process.
  - **Dispatcher**: Manages the dispatching of blocks to specific models, injects positional context, and coordinates with the master model for the next steps.
  - **Contextual Inference**: A shared context mechanism allows all models to maintain coherence and continuity across generated blocks.
  - **Asynchronous Training**: Training is conducted asynchronously by batching blocks of the same type, optimizing resource use.

## 2. Block Structure
- **Fixed-Shape Blocks for Variable Data Sizes**:
  - **Purpose**: Each data mode is processed in blocks of fixed shape, but the size of the content generated within these blocks can vary. This design enables the system to generate data of different sizes (e.g., varying image resolutions) while maintaining a standardized block format for processing.
  - **Maximum Size Constraint**: Each block’s dimensions are constrained so that the product of its dimensions does not exceed a predefined maximum size, ensuring that blocks remain manageable.

- **Mode-Specific Blocks**:
  - **Metadata**: Each block is associated with a specific mode (e.g., text, image, audio) and includes metadata indicating the mode type and the expected size.
  - **Dynamic Shape Management**: Padding can be introduced during training to accommodate dynamic shapes within the fixed block structure, ensuring consistent processing across varying data sizes.
  - **Positional Encodings**: The block schema includes positional encodings for the shape, which are generated by the Dispatcher and provided as part of each block.

## 3. Master Model
- **Mode Prediction**:
  - **Core Function**: The master model is responsible for predicting the next mode to generate based on the common context, which includes information from previously generated blocks.
  - **Adaptation**: The master model must adapt its predictions according to the sequence of modes generated and any overarching patterns in the data.

- **Block Size Prediction**:
  - **Post-Mode Selection**: After selecting the mode, the master model predicts the size of the next block to generate.
  - **Flexibility**: The model must handle varying block sizes, ensuring they fit within the fixed shape constraints while fulfilling content requirements.

## 4. Dispatcher
- **Liaison with the Master Model**:
  - **Block Return**: After a block is processed by a specialized model, the dispatcher consults the master model to determine the next block to generate.
  - **Decision-Making**: The dispatcher uses the master model’s predictions to decide which mode and block size should be generated next.

- **Dispatching and Listening**:
  - **Core Task**: The dispatcher sends blocks to the appropriate asynchronous models for processing and listens for their return.
  - **Model-Specific Handling**: It ensures that each block is sent to the correct model based on the mode, and that the returned block meets the required contract (shape, flattened embedding).

- **Positional Context Injection**:
  - **Contextualization**: Upon receiving a block, the dispatcher injects positional context into the block’s payload. This involves adding sequence and positional embeddings that reflect the block’s position within the overall sequence.
  - **Common Context Update**: The processed block is then concatenated onto the common context, updating it with the latest information.

## 5. Specialized Models and Adaptation Requirements
- **Mode-Specific Models**:
  - **Specialization**: Different models are used for different modes, such as transformers for text and convolutional networks for images.
  - **Integration**: These models are integrated through the common context, which informs each model of prior generated content, guiding the next generation steps.

- **Adaptation Requirements**:
  - **Cross-Attention**: The model must support, or be adapted to support, a collection of embeddings as context, similar to cross-attention mechanisms.
  - **Positional Encodings**: The model should be able to use positional encodings for the shape as provided by the block schema.
  - **Batch Processing**: The model should process data in batches, with masks available to indicate padding.
  - **Adapters**: For models that do not natively support these requirements, adapters may be used to modify them for compatibility with the system.

## 6. Contextual Inference
- **Common Context Mechanism**:
  - **Shared Resource**: The common context is a shared resource used by all models (including the master model) to maintain coherence across generated blocks.
  - **Positional Embeddings**: Sequence and positional embeddings are injected into the common context by the dispatcher, ensuring that all models maintain an accurate understanding of block positions within the sequence.

- **Context Management**:
  - **Efficiency**: Implement strategies to manage the common context effectively, such as context windowing or summarization, to prevent it from becoming too large or unwieldy.
  - **Inference**: The common context should provide enough information to allow models to make informed predictions about future blocks while ensuring continuity across the entire sequence.

## 7. Asynchronous Training
- **Batch Formation and Processing**:
  - **Batching**: Blocks of the same type (e.g., all text blocks) are collected asynchronously until a batch is formed for processing.
  - **Processing and Return**: Once a batch is processed, the resulting blocks are evaluated for loss, broken apart, and sent back to the dispatcher.

- **Dispatcher Coordination**:
  - **Post-Processing**: After processing, the dispatcher, in consultation with the master model, predicts and dispatches the next block to be generated.
  - **Synchronization**: The dispatcher manages synchronization and consistency across batches, ensuring that each block fits seamlessly into the overall sequence.

## 8. Async Model Buffer Mechanism
- **AsyncModelBuffer**:
  - **Purpose**: Each specialized model is placed in an AsyncModelBuffer, which listens for blocks and tries to collect enough to form a batch.
  - **Batch Collection**: Once a batch size is reached, the buffer processes the batch and returns the results to the dispatcher.
  - **Timeout Handling**: If a batch size is not reached within a certain time, the buffer will process whatever blocks it has and return the results.
  - **Synchronous Mode**: The buffer can also operate in a synchronous mode, where it immediately processes and returns a block without waiting for a full batch.


## 9. Training Data Preparation
- **Training Blocks**:
  - Training data is prepared as a list of "training blocks," each containing the following elements:
    - **Mode**: The mode of the block (e.g., text, image, audio).
    - **Shape**: The ND shape of the block, defining the structure of the data within it.
    - **Payload**: The autoregressive teacher-forcing context, which includes embeddings or patches representing the content we want to generate.
    - **Targets**: The loss targets, with the possibility of having more than one (e.g., for tasks like upsampling).
    - **Loss Function**: The specific loss function to use, which must return a loss per batch.
    - **Callback**: A callback function used to pass back the block results, the computed loss, and any metrics to monitor.
    - **Timeout**: When this is reached, we force execution of the block. 
- **Training Process**:
  - These training blocks are handed to the dispatcher for dispatching and management, guiding all the necessary steps to complete the training process.

## 10. Evaluation
- **Evaluation Blocks**:
  - Evaluation blocks are prepared similarly to training blocks but are distinguished by having no targets. These blocks represent the context we wish to prompt the model with during evaluation.
  
- **Prompting and Generation**:
  - **Context Injection**: When prompting the model, the dispatcher injects context into each payload, concatenates them together, and manages the inference process.
  - **Sampling and Generation**: Instead of teacher-forcing, the master model samples from its known mode and size distributions to determine the next mode and block size to generate. This sampling process is guided by a "temperature" parameter to control the randomness and diversity of the generated content.
  - **Block Generation**: Blocks are generated as usual, with the models creating content based on the context and the sampled predictions made during the evaluation process.

# Implementation

Here we start to get into the details of implementing the project. Information
after this point should be considered more current.

## Data Structures

### Schema

**Purpose**

The schema data is used to store and understand shaped information. In
particular, it is used to store the maximum size a given dimension can reach.

A schema is a positive int tensor in N dimensions. Each integer defines 
how many logits to assign to a particular mode or shape dimension. For
instance, a schema of [100, 100] might be used to specify that an
image with shape up to 100 on x and 100 on y, will be generated. In total,
200 units of logit are needed.

The schema basically defines what the maximum values of ND data can be. It is expected 
that each dimension will in turn be predicted by a logit mechanism, which can be 
sampled from.

**Attributes**

* mode [str]: The mode the specific schema is associated with
* schema [torch.Tensor]: A 1d torch tensor specifying the schema 
* length [int] [property]: The schema length in logit space. Inferred from product of schema

### BlockDemand

**Purpose**

The block demand is the fundamental datastructure of the asynchronous process. It represents
a single unit of context to generate in a certain mode, and is used during evaluation
and training. During training, the block is given targets, while eval leaves them
blank. 

Blocks are dispatched to asycronous execution units that will accumulate blocks
until they have a batch, then run them all at once. 

**Attributes**

* Id [Int]: A unique block ID. 
* Mode [str]: This is a string that associates the mode with the 
* Shape [torch.Tensor]: The shape of this feature. Should not exceed schema.
* Sequence [Int]: The block sequence position.
* Targets [torch.Tensor]: Encoding targets. Assumed to be a flat grid of integers. Optional
* Context [torch.Tensor]: The common context for the sequence so far.
* Loss function [Callable]: The loss function to use when training. 
* Timeout [float]: A timeout in milliseconds. Will schedule an interrupt that forces block completion.
* Callback [Callable]: A callback that is called with the results once the block is finished processing.

### BlockResponse

**Purpose**

The block response is the return that is passed back to the dispatcher once
a given block has been processed. It contains the common context that needs 
to be integrated, along with some additional information

**Attributes**

* Id [Int]: The unique block id we are responding to
* Context [torch.Tensor]: The encoded context we are providing.
* Loss [torch.Tensor]: A singular loss tensor associated with the block. Optional, and 
                       not used when evaluating.
* Metrics [Dict]: Any metrics we are trying to monitor.


## Helper Classes

Helper classes contain some level of logic. They are more than just a datastructure,
but also not a layer.

### SchemaReader

**Purpose**

The schema is an encapselation of the schema data, alongside
with methods to work on said data. This includes being able to
validate, along with actual extraction logic.

**Dependencies

* SchemaData: Contains the actual data

#### Method: validate_schema

**Purpose**

Validates that a particular schema is compatible with a particular
control model

**Accepts**

* logits_length [int]: The length of the logits tensor

**Raises**:

* ValueError: If the length of the schema exceeds the logit length

#### Method: validate_target

**Purpose**

Validates that a particular shape target is compatible with the schema

**Accepts**

* target [torch.Tensor]: A specific target

**Raises**

* runtime_error: If the targets are negative.
* runtime_error: If one of the targets exceeds the maximum length

#### Method: extract_by_schema

**Purpose**

Extract logits from a tensor into multiple independent dimensions
while operating under the assumption that the logits are specifying
the schema elements, in order. Assumes the shared logit space
mechanism

The shared logit space mechanism would, for a schema of [3, 2], associate
the first 3 elements of a logit with the first dimension, and the next 2 
with the second dimension. This lets us compactly represent multidimensional
data using the same control model

**Accepts**

* logits: A logit tensor to extract under a given schema.
  * Shape (..., logits)

**Returns**

* separated_logits:
  * The logits separated by dimensions
  * Shape (..., D, shorter_logits)
* mask:
  * The mask indicating what are padding elements
  * Shape (..., D, shorter_logits)

** Design**

Basically, already done. Reuse existing code from other project.


## Layers: Adapters

Adapters are utilized in order to ensure compatibility between the generation
cycle and individual modes of operation. There are a few specific adapters,
then adapter "plugs" that can be placed within a block buffer to get things done

### ContextAdapter

**Purpose**

The context adapter is responsible for converting the context of the block 
into something that the control model will understand. Each mode needs it's 
own context adapter due to the fact that the way shape information needs to
be encoded varies for each mode. 

**Dependencies**

* mode: The mode to build the context adapter in.
* config: The config will end up being important. We will need:
  * schema: The schema for the mode
  * context_dim: The size of the context dimension embedding.

**Accepts**

The forward method accepts:

* sequence [torch.Tensor]: The sequence integer of the block. Scalar
* shape [torch.Tensor]: The shape tensor. 1d.

**Return**

The adapter should return

* context_encoding [torch.Tensor]:
  * Encodes the context the block was generated in
  * Shape (context_dim)

**Design**

It is probably easiest to just feed the sequence integer, and shape integers,
into a linear projection of context_dim shape. The bias would be sufficient 
to encode the mode. However, more complicated options such as sinosoidal 
positional encodings could also be explored.

### ModelAdapter

**Purpose**

The model adapters purpose is, unsuprisingly, to be an adapter that makes
models compatible with the broader context. It is targetted towards 
autoregressive transformer decoders, though other contexts might work as
well.

An ideal model to work with would be a transformer encoder-decoder. The 
decoder can 

TODO: CONTINUEEEEEEEEEEEEEEEEEEEEe

## ModelAdapter

**Purpose**

The Model Adapter's purpose is to be an abstract class which when implemented
can be bound to a specific model and prove able to train and sample from that model.
It is a adapter, in other words.

**Dependencies**

* Must be implemented to be used
* Mode [str]: The mode to associate with this model.

### method: setup

**Purpose**

The setup method is responsible for, suprise suprise, setting up a 
model that is compatible. When passed a dictionary specifying details
on the environment and desires, it should pass back an instance bound
to the environment. It is abstract

**Accepts**

* config:
  * A dictionary containing config details.

**Returns**

* a ModelAdapter subclass

**Design**

The design is going to depend on the adapter implementation, obviously, but should likely
include details such as setting up adapter layers to convert between the internal embedding
shape and the common context shape, etc.


### method: encode_context:

**Purpose**

The encode_context method does as it's name implies - it attempts
to encode context. This method will be called once the generated
data is known, and should attempt to return context that will be useful
for the common context mechanism

**Accepts**

* data: The flattened intgrid data. Either the targets, or the predictions
* data_padding_mask: What elements of data were padding
* positional_encodings: Positional encodings to mask the data

**Returns**

* common_context: The common context embeddings, with whatever we want to 
                  say about them.

**Design**

This will, most probably, just be a transformer encoder. However, 
it will be designed to be specific to the scenario at hand, and it 
will need to be able to encode either the targets or the predictions
directly. It will almost certaintly need to share vocabulary or the
equivalent with the decoding methods.

Encoding is, conceptually, done immediately after decoding finishes
for the block, and happens at this point rather than when the next
block begins to read in order to ensure information in various modes 
and with various shapes can still be represented in terms of the 
common embeddings format.

### method: predict_distributions

**Purpose**

The predict distributions mechanism is responsible for predicting
the training distributions we are trying to learn. It is used 
during training with teacher-forcing.

**Accepts**

* targets: The flattened intgrid targets, in a batch
* targets_mask: A mask indicating what part of the targets is padding
* common_context: The encoded common context. Batched
* context_padding_mask: The padding elements of the common context.
* positional_encodings: Positional encodings to suite the size.

**Returns**

* distributions: A tuple of tensors defining the various distributions under consideration. 
  * Of significant note, header dimensions must be the same as targets. This assigns that part
    of the distribution to the target
  * Some distributions, such as logit probabilities, can be defined with one tensor. Others,
    like gaussian mixture models, need multiple.
* metrics: Exactly what it sounds like

**Design**

We basically run whatever model we have, with teacher forcing, in order to make 
predictive distributions. Nothing more to it, really. 

One special concern that needs to be brought up is padding with multidimensional
data. In these cases, we can end up with things like one image that is 100x10, and another
which is 10 x100, which when put together would require something that is over 99% padding if
naively combined. That risk is too high.

Instead, we force all data to flow into the generative process in a flat format, and provide
positional encodings to maintain position. This means we only require an ammount of padding
that is associated with the numel of the grids. We can revisit concepts like PixelCNN if 
performance is too low.

Transformers are probably the only real generative option. Fortunately, someone has used
transformers to generate basically every mode at this point.

### method: sample_distribution:

**Purpose**

The sample distribution mechanism has the job of extracting a sample
from the model by next sequence prediction. It runs the entire sampling
process and returns the result.

**Accepts**

* common_context: The batched common context tensor. 
* context_padding_mask: The mask for the context indicating what was padding
* positional_encodings: Flattend positional encodings. These have the shape of 
                        the output tensor, actually.
* temperature: The temperature we are sampling at. Controls diversity

**Returns**

* predictions: Flat prediction data for each element in the block.
* distributions: The distributions, in case we do anything with them
* metrics: Any metrics we would like to record. 

**Design**

The exactly details will, naturally, depend on the distribution under consideration.
However, the final output is not up for debate. We should end up running a sampling
process over the distributions and sampling from them, based on the temperature.


## AsyncBlockBuffer

**Purpose**

The async block buffer is designed to cache blocks and attempt to 
assemble batches that are efficient - that is, that have fairly
minimal padding. 

It also needs to keep an eye on timeouts. When a block times out, it
will automatically get the cluster belonging to the batch that has 
timed out, then execute that batch. 

Note that separate block buffers are maintained for training vs evaluation,
but the separation occurs at a higher level. 

**Dependencies**

* batch_size: the size of the batches to try to make.
* execute_batch: call it to execute a batch and handle the returns

**Overview**

Basically, we end up with two processes. The forward method
is capable of caching the blocks and putting them together
into batches - when a batch is done, it also then dispatches 
it to a callback. 

Additionally, e

### method make_batch:

**Purpose**

Creates a batch out of the specified blocks. This means hunting down the maximum

### method forward

**purpose**

This method's purpose in life is to handle block dispatches. 

Every time a block is dispatched to this AsyncBlockBuffer, this
method is called. It then integrates the 

### AsyncModelBuffer

**Purpose**

The async model buffer is basically an entire training pipeline,
specific to a certain kind of data, all located in one location. 

In terms of IO, it is responsible for:

* asyncronous block accumulation
  * Accumulate blocks dispatched to it in order to put together effective batches
* asyncronous results: 
  * Provide results back to dispatcher once a batch has been processed.

Internally, it needs to handle:

* block caching
* batch creation
* batch running
* loss evaluation
* metric storage


s responsible for containing blocks during the async batch
accumulation state, and for putting the blocks together into reasonably efficient batches. 

It is also responsible for taking the results back apart and returning them to the dispatcher.
Finally, it is also 


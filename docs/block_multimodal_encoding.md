# Block Multimodal Encoding

TODO: Include section definitions

## What is it?

Block Multimodal Encoding is an encoding mechanism designed to represent
data across different modes within an encoding or decoding transformer-like
context. This approach allows a single model to handle sequence-based
multimodal data, accommodating various data types such as text, voice, or
images.

It has been designed in response to the ARC-AGI challenge.

## How does it work?

Suppose we have a sequence of $N$ different multimodal data pieces we
wish to encode together. For example, consider a sequence consisting of text,
followed by an image, and then more text. This forms a series of blocks: a
Text block, an Image block, and another Text block.

Data is encoded or decoded in large contiguous segments called "Mode Blocks."
Each Mode Block consists of a Header section and a Payload section, each
containing a series of embeddings. The Header section includes a Mode
embedding, a Shape embedding, and a Sequence embedding. Each Block has a
tensor shape like:

$[\text{mode\_header}, \text{shape\_header}, \text{sequence\_header}, \text{payload} + \text{context}\ldots]$

### Headers

The headers are the three embeddings at the beginning of a given Mode Block:
Mode, Shape, and Sequence.

- **Mode Header**: An embedding that specifies the mode of operation (e.g.,
  text, image). There are $N+1$ integers associated with modes,
  representing different content types and an END mode, analogous to an EOS
  token.

  - **Shape Header**: Each mode has specific Shape tensor predictors and
    embedding systems. A shape tensor, such as $[100]$ or $[10, 10]$,
    indicates the structure of the data, like a string of 100 tokens or a 10x10
    image. This tensor helps determine the payload length, calculated as the
    product of its dimensions.

  - **Sequence Header**: An embedding indicating the block's position within
    the sequence. The first block is generated with a sequence tensor set to 0,
    the second to 1, and so forth, distinguishing between blocks.

These header embeddings are generated by the model as targets, similar to
tokens, and define multidimensional data with dynamic shapes.

### Payload

The payload is the actual content that needs to be read or generated. Each
mode has its own payload embedding mechanism, loss function, and prediction
strategy during decoding. In training, a payload starts as raw data, which is
then embedded and used as a prediction target for the next sequence.

The payload itself lacks positional information, which is provided by the
context.

### Context

The context provides additional information, such as position and mode,
injected into each generated payload feature. It consists of:

- **Header Context**: Derived by summing the three headers, this context is
  added to each payload tensor, ensuring the model understands the mode,
  shape, and sequence details.

  - **Positional Context**: Created using the Shape tensor, this context
    consists of flattened multidimensional embeddings. These embeddings are
    generated on-the-fly during decoding to inject positional context into
    produced tensors, enabling the model to infer position-based insights.

For instance, a 100x100 image could have 100x100 multidimensional positional
encodings, which are then flattened into a singular embedding of shape 10,000.

## Common Context 

Models generate in a 

## Data and Embeddings

In the Block Multimodal Encoding system, we process a sequence of multimodal
data into Block Tensors, which are then used to generate targets for training
and teacher-forcing embeddings. This setup ensures efficient processing and
consistency across different modes.

### Multimodal Sequence

The sequence consists of $N$ different data pieces, each potentially in a
different mode, such as text, image, or audio. For each piece of multimodal
information, the model extracts necessary features to convert the data into
processable tensors.

### Block Tensors

From each multimodal data piece, we derive four Block Tensors to guide the
model's understanding and processing:

1. **Mode Tensor**
   - **Purpose**: Identifies the mode (e.g., text, image) of the data piece.
   - **Shape**: $(B, 1)$, where $B$ is the batch size.
   - **Structure**: An integer representing the mode type, associated with a
     specific embedding.

2. **Shape Tensor**
   - **Purpose**: Describes the dimensions of the data piece, informing the
     model of its structure.
   - **Shape**: $(B, D)$, where $D$ is the number of dimensions.
   - **Structure**: A tensor of integers indicating the shape (e.g.,
     dimensions of an image or length of a text sequence).

3. **Sequence Tensor**
   - **Purpose**: Specifies the position of the data piece within the
     multimodal sequence.
   - **Shape**: $(B, 1)$
   - **Structure**: An integer indicating the order of the block in the
     sequence.

### Target Generation

For each mode, we produce targets used for loss calculation. The targets are
generated to efficiently use shared logits, minimizing the need to store
multiple distributions during training.

- **Logit Structure**: The output logit consists of $K$ entries, where each
  mode can assign these for different purposes. For instance, a mode might use
  all $K$ for vocabulary (text generation) or allocate $K = L + N$, where $L$
  and $N$ are dimensions for specific functions.

  - **Targets**: The targets are $N$-dimensional, specifying the intended output
    for each region. Modes must define how they use the combined logit space,
    and any unused targets are set to a negative value to indicate lack of use.
    Additionally, targets will be integers representing position in the logit
    space based on the schema, and should be defined since the start of the 
    logit not the start of the schema zone.

  - **Contract for Modes**: Each mode must specify how it uses the shared logit
    space, detailing how $K$ entries are allocated and how to handle unused
    targets.

### Teacher-Forcing Embeddings

During training, embeddings for each Mode Block are generated with
teacher-forcing, involving the following steps:

1. **Embedding the Mode**: Transform the mode integer into a learned embedding
   vector.

   2. **Embedding the Shape**: Convert the shape tensor into an embedding that
      captures the dimensions and layout of the data piece.

   3. **Embedding the Sequence**: Translate the sequence integer into an
      embedding that reflects its position in the multimodal sequence.

   4. **Embedding the Payload**: Each payload entry is transformed into an
      embedding. For images, this might involve tokenizing pixels; for text, this
      could mean tokenizing words or characters.

These embeddings are produced initially without context injection. Context
injection occurs in later processing stages, allowing the model to focus on
the inherent properties of the data first.

### Loss Modes and Calculation

The model operates with multiple loss modes, facilitated by the Loss Select
Tensor. This tensor guides the model in determining which loss function to
apply based on the operation being performed:

- **Mode Loss**: Applied when generating a mode.
  - **Shape Loss**: Each of the $N$ non-end modes requires shape generation
    conditions.
  - **Payload Loss**: Each of the $N$ modes has a specific payload processing
    condition.
  - **Padding Loss**: No loss is applied when in padding mode.

These modes are encoded using an integer tensor specifying one of the above
conditions. If calculated correctly, there are $2 + 2N$ states to handle. The
loss function uses this information to evaluate the appropriate loss.

### Process Summary

- **Input Sequence**: $N$ multimodal data pieces, each with a unique mode.
  - **Transformation**: Extract Mode, Shape, Sequence, and Loss Select tensors.
  - **Target Generation**: Create $N$-dimensional targets using shared logits.
  - **Embedding Generation**: Produce teacher-forced embeddings for the entire
    sequence.

This structured approach ensures each data piece is consistently transformed
into tensors, facilitating efficient model training and adaptability across
various modes.

## Shared Logit Space

The shared logit space is a unified tensor structure produced by a single
feedforward network. It allows the model to efficiently handle multiple modes
by organizing logits into a common framework, facilitating seamless integration
of various distributions.

### Schema

A schema defines how the shared logit space is partitioned for each mode and
distribution. It allocates specific portions of the logits to different tasks,
ensuring that each mode uses the logits appropriately according to its needs.

- **Definition**: Each mode has a schema specifying the allocation of logits
  across its various dimensions and functions.
  - **Function**: Schemas dictate which parts of the logit space correspond to
    specific outputs, enabling the model to differentiate between modes and
    ensure the correct logits are used for each task.
  - **Example**: The schema $[3, 4]$ indicates that the first 3 elements of the
    logit tensor are dedicated to the first target (i.e., a distribution with 3
    probabilities), and the next 4 are for the second target.

### Targets

Targets represent the desired outcomes for each mode and are used during
training to guide the model's predictions.

- **Definition**: Targets are $N$-dimensional vectors indicating the expected
  outputs for a given input. They are defined with respect to the start of the
  schema. For instance, with schema $[3, 4]$, you could specify a target within
  the second distribution as $[*, 4]$, but not $[*, 7]$.
  - **Mapping**: Targets are mapped onto the shared logit space by adjusting
    them to reflect absolute positions from the start of the logit. This involves
    adding the cumulative schema length to the targets.

### Mapping Targets onto the Shared Logit Space

Hereâ€™s an example of converting targets to logit targets and back:

1. **Schema**: Consider schema $[4, 7]$ with targets $[3, 5]$.
   
   2. **Cumulative Schema Length**: Calculate the cumulative schema length tensor:
      - $[0, 4]$

   3. **Mapping to Logit Space**: Add the cumulative schema length to the targets:
      - Targets in logit space: $[3 + 0, 5 + 4] = [3, 9]$

   4. **Reverse Mapping**: Subtract the cumulative schema length to revert to the
      original target format:
      - Reverted targets: $[3, 9] - [0, 4] = [3, 5]$

This mapping process ensures that targets are aligned correctly with their
respective logits, allowing efficient loss calculation and sampling across
modes.


## The Generative Process

The Block Multimodal Encoding system uses a cyclical process to generate data,
consisting of four key steps. This cycle is repeated during both training and
evaluation.

### Steps in the Generative Process

1. **Mode Select**: Determine the mode for the next block. The model projects
   the current state into a probability distribution over possible modes, and
   the selected mode's embedding is integrated into the output stream.

   2. **Shape Select**: Predict or select the shape of the output for the chosen
      mode. This step generates a shape tensor and its corresponding embedding to
      define the payload's structure. It concludes with the model knowing the
      shape of the output space.

   3. **Sequence Synthesis**: Establish the block's position within the sequence
      and generate a sequence embedding to maintain order and provide context for
      subsequent steps.

   4. **Payload Processing**: Generate or process the payload data. Compute
      positional encodings based on the block's location and shape, injecting
      context as needed. There must be an established mode, shape, and sequence
      before the context for payload processing can be defined.

### Model switching

One possible feature that could be explored, though is not initially planned
to be, is model switching. It is entirely possible to specify different
models for the different generative context, and simply let them see the same
shared embeddings.

This might be explored later. The design for it is quite clear, however. You
simply need a Mode Control model, and N different mode generation models. You perform
header generation under mode control, then switch to the payload generation models.

### Logits and Probability Distributions

The model operates with a shared logit space, meaning a single feedforward
network produces a unified tensor of logits for all modes. This shared logit
space ensures that all distributions, despite differing contexts, coexist
within the same tensor structure.

- **Mode Logits**: Indicate the likelihood of each mode. The model uses these
  logits to decide the mode to generate next, based on the current state.

  - **Shape Logits**: Represent probabilities for each dimension of the shape.
    These logits help predict or select the shape of the output space for the
    current mode.

  - **Sequence Logits**: Although generated as part of the process, sequence
    logits do not influence the mode or shape selection. The sequence position
    is teacher-forced to correspond to the block number, ensuring consistent
    ordering without relying on a distribution.

  - **Payload Logits**: Guide the generation or interpretation of each token or
    unit within the payload. They play a critical role in ensuring that the
    content generated or processed is aligned with the expected output.

### Training Process

During training, the model:

- Stores distributions for loss calculation.
  - Uses targets specific to modes, shapes, and sequences to guide selection
    through teacher-forcing.
  - Applies context through the sum of mode, shape, and sequence headers.

### Evaluation Process

During evaluation, the model:

- Samples from the probability distributions to make decisions at each step.
  - Relies on the generated distributions to autonomously determine the mode,
    shape, and payload processing.

### Process Overview

This generative process efficiently manages data generation by using logits to
make informed decisions and injecting context to ensure consistency across
various modes and structures. The cycle repeats, continually selecting new
modes and processing subsequent blocks.

##

## Losses and Distributions

In this model, distributions are defined in a unique way, leveraging a shared
logit space across different modes. This section describes the mechanisms
employed to handle loss calculation and sampling efficiently.

### Schema Tensor

The Schema Tensor defines the loss schema for each embedding and allocates
portions of the shared logit space to various distributions.

- **Purpose**: Indicates how the logits are partitioned for each distribution.
  - **Structure**: Each distribution has its own schema tensor.
  - **Shape**: $(\text{batch} \times \text{distributions} \times D)$, where $D$
    represents the dimensionality of the schema.

### Associated Schema Tensor

The Associated Schema Tensor links a schema with every embedding generated by
the model.

- **Purpose**: Provides a mapping from embeddings to their corresponding loss
  schemas.
  - **Generation**: Created by using the Loss Select Tensor to sample vectors
    along the distribution dimensions.

### Logit Separation Mask

The Logit Separation Mask separates the different distributions into distinct
tensor indices, enabling the application of cross-entropy and similar loss
functions.

- **Purpose**: Segments the shared logit space into target dimensions.
  - **Structure**: A sequence of repeated boolean values that demarcate logit
    indices for each target dimension.

For example, with a schema of $[3, 2]$ and $D=2$, the resulting tensor might
look like this:

$$
\begin{align*}
\text{[True, True, True, False, False],} \\
\text{[False, False, False, True, True]}
\end{align*}
$$

Note that in the case of an unused dimension, the logit separation mask will just
be filled with zeros.

### Cross Entropy

Cross entropy loss can be applied extremely straightforwardly by using the above information.

We proceed to use torch's cross entropy, with even label smoothing, and use the ignore_index
feature plus the separation mask to ensure that targets with no force are filled with the ignore
index value.

### Sampling

Sampling from the model's outputs operates similarly to standard methods,
leveraging the partitioned logit space:

- **Method**: Sample from each individual target channel separately.
  - **Consistency**: This approach maintains consistency across modes by
    respecting the segmentation of logits into distinct channels.
  - **Adjustment**: Once the targets are specified, we need to subtract the
    cumulative schema lengths to revert to the original format.


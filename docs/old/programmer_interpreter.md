# NOTE:

This has been AI generated. I need to go rewrite it as a tight spec

# ARC-AGI Challenge: Programmer and Interpreter Specification

## 1. Programmer Specification

### Overview
The Programmer is responsible for generating a sequence of instructions (a program) that will be stored in the Neural Turing Machine (NTM) memory. This program is designed to map inputs to outputs based on the correlations observed in the example collection.

### Inputs
- **Input Collection**: A set of input data sequences or grids.
- **Output Collection**: Corresponding target outputs for each input in the collection.

### Outputs
- **NTM Memory Program**: A sequence of instructions stored in the NTM memory.

### Core Functions
1. **Fixed-Width Memory Setup**:
   - **NTM Memory Space**: Set up a fixed-width memory space for the NTM, ensuring that it has a predefined structure for storing the program.
   - **NTM Write Heads**: Configure a fixed number of write heads in the NTM, which will be used to write the program into memory.

2. **Transformer Decoder Model**:
   - The Programmer will be implemented as a transformer decoder model, with the input example collection serving as the context.
   - The decoder will process the input examples and produce program embeddings, which represent predictions for the next program command.

3. **Program Command Generation**:
   - **Program Embeddings**: The decoder generates embeddings that encapsulate the predicted program command for each step.
   - **Program Command Interpretation**: The embeddings are then interpreted into actual program commands, which are saved by the NTM system.
   - **Vocabulary Insertion** (Optional): Introduce a vocabulary layer between the program embeddings and the program commands. This layer will:
     - Limit the NTM system to a fixed number of commands, ensuring consistency and simplicity.
     - Encourage the model to generalize commands rather than relying on specific references (e.g., avoiding case-specific details like exact cell numbers).

4. **Memory Read Access During Programming** (Optional):
   - Allow the Programmer to read from the NTM memory while generating the program. This might involve:
     - Adding a separate NTM read head (or multiple read heads) that the Programmer can use during the programming phase.
     - Enabling the model to make more informed decisions by consulting the memory as it programs, potentially improving the quality of the generated program.

5. **Learning and Refinement**:
   - The Programmer will be trained iteratively, refining the programs it generates based on the performance of the Interpreter.

## 2. Interpreter Specification

### Overview
The Interpreter is responsible for executing the program stored in the NTM memory to map inputs to outputs. It acts as the executor of the instructions generated by the Programmer.

### Inputs
- **Input State**: The current state of the input data.
- **NTM Memory**: The programmed memory containing the sequence of instructions generated by the Programmer.

### Outputs
- **Generated Output**: The final output produced by the model, which should match the target output specified in training.

### Core Functions
1. **Initialization**:
   - Load the input state and initialize the program counter or other control mechanisms.

2. **NTM Memory Access**:
   - Use the NTM read/write heads to access the program stored in memory.

3. **Instruction Execution**:
   - Execute the instructions retrieved from the NTM memory, modifying the input state as directed.
   - Utilize ACT to allow the Interpreter to execute the program for as many steps as needed.

4. **Context Management**:
   - Focus on specific parts of the input context as directed by the program.

5. **Output Generation**:
   - Produce the final output and compare it to the target output during training.

6. **Feedback Loop**:
   - Backpropagate errors to refine both the Interpreter’s execution and the Programmer’s program generation.

## Integration of Programmer and Interpreter

- **Programming Phase**: The Programmer generates the NTM memory program based on the entire input-output collection.
  
- **Processing Phase**: The Interpreter uses the programmed NTM memory to process new input states and produce outputs.

## Conclusion

This specification outlines a clear framework for developing a system that leverages the strengths of both transformers and NTMs to tackle the ARC-AGI challenge. The modular design allows for flexibility and optimization, making it a promising approach for complex, algorithmic tasks.


# Overview

We view transform creation and manipulation to be about decoding a given instruction from the cognative space into
a transform, and providing pertinent and useful feedback about transforms that have already been decoded. We have
two primary concerns to worry about

* Creating transform directives
* Executing transform directives and maintaining the associated latent space
* Evaluating transform queries based on the latent space.
* Encoding feedback and loss based on the latent space.

## What is a transform directive?

At the heart of everything in this section is the idea of the transform directive, and recieving 
feedback from them. Lets start by discussing that.

A transform directive is a tensor collection consisting of *instructions*,  *return probabilities*,
and *mask* that can be interpreted by a *transform engine*. The *instructions* can be thought of a sequence 
of instructions on what to do and in what order. Meanwhile the *return probabilities* are probabilities
that represent the likelyhood of this step being included as an output of the transform engine. Due
to the fact that generation may stop at different moments on different tensors, the *mask* indicates
which instructions are active.

## What are their shapes?

A transform instruction has a shape:

* instruction: Shape (batch x embedding)

A collections of instructions thus has shape

* instructions: Shape (batch x instructions x embedding)

While return probabilities have elements strictly between 0 and 1.

* return_probabilities: Shape (batch x instruction)

Concisely stated, the transform_directive is:

* transform_directive
  * instructions: Shape (batch x instructions x embedding)
  * return_probabilities: Shape (batch x instructions)

## What are their technical meanings and underpinnings?

Each *instruction* correlates directly with a transform of some sort to be applied to the 
latent space. All instructions will be executed in the given order to produce latent space transforms.
However, not all results will be returned, as sometimes it might take multiple computations to get a single
transform done. The downstream implementation needs to handle this, and is tied to the actual model.

**result slot assignment**

Determining the number of results, and the probability of a transform ending up in a particular slot, are
the purpose of the *return_probabilities*. A given output falling out of the *transform engine* will
have a number of outputs specified by the *return_probabilities*. In particular, it will be equal to the 
sum of the return probabilities, which will always be a whole number. 

Then, the question should be posed, how are outputs, which could be much shorter than the
the transform directive, constructed from the discrete transform results? Imagine a sequence
of N outputs. Starting at the first return slot, we generate results, weight them by their
return probability, then add it to the return slot. We also keep track of how much probability
is in each slot. Once the slot is filled all the way up to 1.0 probability, we consider it done
and move onto the next slot.

A process during generation of the transform directive ensures the sum of items in the slot
always ends up adding up to one.

## Generating a Transform Directive

### Cognitive input

A transform directive starts life as a cognition_command tensor and a cognition_context tensor. The
Lets start by considering these. These tensors are inputs from the cognition engine, and respectively
say what we want to do, and what might help us do it. Concisely summarized:

* cognition_command: 
  * Generated by cognition engine, can be thought of as a command to get things done.
  * Shape (batch x embedding)
* cognition_context:
  * A sequence of tools that may be relevant. From the cognition engine
  * Shape (batch x N x embedding)
  * We cannot predict the shape of N

### Generating instructions using a Generative Decoder Transformer

Instructions are generated one at a time using a generative transformer decoder. 

The context is provided by the cognition context, and the initial embedding is fed in in leu
of a "Start_Of_Sequence" token. The output is now the first "instruction" in the transform directive.

We then concatenate that onto our input, feed THAT back into the model, and keep repeating this 
process until we are done generating. This happens in a manner reminiscent of LLM's used for natural 
language processing. However, unlike that application, we do not predict to a vocabulary and decode, but
instead feed the output embedding back in directly. Nonetheless, the idea of next sequence prediction
is still pretty valid.

### Generating the associated return probabilities

The return probabilities are generated by means of a logit projection followed by a sigmoid activation. 
Additionally, a running tally of the probability in the current output slot is kept, the cumulative_return_probability

When the return probability + cumulative_return_probability < 1.0 - epsilon, nothing in particular happens.
However, if it is GREATER than that, we end up clamping the return probability so the cumulative probabilities
cannot exceed one by setting it equal to 1 - cumulative_return_probability

After this computation is complete, it is then clamped again to the minimum between the revised
return probability and the cumulative halting probabilities. This will be explained more in the 
next section, but hopefully has the effect of encouraging the model to not produce halting
probability until it is actually ready to stop. Else, it is limited in how strongly it can propose the 
correct answer!

### Stopping generation and Halting Probabilities

We borrow a trick from Adaptive Computation Time to decide when to "stop" autoregressively generating.
interpreted instructions. We assign each batch a cumulative halting probability. We say that when the cumulative
halting probabilities reach 1 for a batch, that batch is in a halted state. 

Then each time we finish generating instructions leading to an output - that is, each time
we max out the return probability - we create a halting probability out of the final instruction.
This halting probability adds to the cumulative halting probability, which then clamps the return probabilities

Once the halting probability is exhausted, it continues to clamp the return probabilities. This
will mean in addition that losses targeted at the return probabilities will be capable of encouraging
the model to delay the production of halting probability.


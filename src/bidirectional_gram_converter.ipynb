{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786360fd22dfba7d",
   "metadata": {},
   "source": [
    "# Bidirectional Gram Converter\n",
    "\n",
    "We will use the Bidirection Gram Grid Converter architecture to convert between gram embeddings and the original representation. This uses autoencoding for initial training, and makes decoding the opposite of encoding. It also encourages the model to encode into the gram encodings\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* If each encoder cell uses a gram embedding to create an update then subtracts it from the input, each decoder cell can reverse it using that same layer to remake the update and then add it.\n",
    "* We can encourage the model to shunt it's information through gram embeddings instead of the main embeddings with the proper loss on the encoder output.\n",
    "* By sprinkling in positional embeddings regularly before convolutions, we can let the model retain the ability to access to positional information.\n",
    "\n",
    "**Design**\n",
    "\n",
    " At a overall level, the encoder/decoder model acts a lot like a ResNet. It consists of cells with residual bypasses that operate more or less on pixel embeddings - however each cell produces both the next pixel embedding in the chain, AND a Gram Encoding of the layer. \n",
    " \n",
    "However, the cells themselves are quite sophisticated, and are designed to allow bidirectional encoding-decoding with almost exactly the same parameters. \n",
    "\n",
    "A BResNet cell has three main components. These are.\n",
    "\n",
    "* Latent Summary stage: Uses internal parameters and actions to create a Gram Encoding. \n",
    "* Latent Decode stage: Uses a provided Gram Encoding and a known grid shape to create an update of the same shape as the summary input.\n",
    "* Merge stage: Either add or subtract the update. Add when encoding. Subtract when decoding. Replaces the residual bypass: Here is your residual now.\n",
    "\n",
    "The effect of this is that the distinction between encoding, and decoding, from the model's perspective is only a distintion in whether you subtract and use the summary stage, or add and use external Gram Encodings. Lets consider one particular cell operating in encoding and decoding mode to see this illustrated\n",
    "\n",
    "**Training**\n",
    "\n",
    "The mechnanism can be trained as an autoencoder. However, we shall motivate the autoencoder to not rely on the encodings output by the encoder by penalizing those encodings when nonzero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37573f92d41cc617",
   "metadata": {},
   "source": [
    "# Hyperparameters and Imports\n",
    "\n",
    "Hyperparameter and imports go here"
   ]
  },
  {
   "cell_type": "code",
   "id": "50c30327298a3381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:15.612078Z",
     "start_time": "2024-08-05T23:43:10.095784Z"
    }
   },
   "source": [
    "import torch\n",
    "import unittest\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from unittest.mock import Mock\n",
    "\n",
    "from typing import Tuple, List, Callable, Union, Any"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4362854209767237",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:15.627702Z",
     "start_time": "2024-08-05T23:43:15.612078Z"
    }
   },
   "source": [
    "# Hyperparameter and search space\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10\n",
    "\n",
    "\n",
    "# Intgrid Encoding\n",
    "\n",
    "\n",
    "\n",
    "num_layers_encoding_cell = 2\n",
    "num_layers_decoding_cell = 2"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "2172698eafe84fae",
   "metadata": {},
   "source": [
    "\n",
    "# Primitive Layers\n",
    "\n",
    "We begin to define the various pieces needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e056b65ea83a7",
   "metadata": {},
   "source": [
    "## Pixel Cell\n",
    "\n",
    "The convolution processing cell is really the only architecture-specific piece here. It is specialized for processing embedded pixel data.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need a convolutional processing mechanism for pixel images.\n",
    "* We will need a cell for that.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `embedding_dim`: The dimensions of the embedding.\n",
    "* `num_layers`: The number of layers deep the cell is.\n",
    "* `kernel_size`: The size of the convolutional kernel.\n",
    "* `dropout_prob`: The probability of an element to be zeroed during dropout (default: 0.5).\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: Image embeddings. Shape (batch x N x M x embedding_dim)\n",
    "* `mask` (optional): A mask to apply at the end. Shape (batch x N x M)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: New image embeddings. Shape (batch x N x M x embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b52b3a7a643e246e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:15.658948Z",
     "start_time": "2024-08-05T23:43:15.627702Z"
    }
   },
   "source": [
    "class PixelCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional cell network. Processes image embeddings without reduction.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 num_layers: int,\n",
    "                 kernel_size: int,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the conv cell\n",
    "        :param embedding_dim: The dimension of the embeddings\n",
    "        :param num_layers: The number of layers to make\n",
    "        :param kernel_size: The kernel size of the convolutional layers\n",
    "        :param dropout_prob: The probability of an element to be zeroed (default: 0.5)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Construct layers\n",
    "        padding_size = (kernel_size - 1) // 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(embedding_dim, embedding_dim, kernel_size, padding=padding_size),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to process embeddings through convolutional layers.\n",
    "        :param embeddings: The input embeddings. Shape (batch x N x M x embedding_dim)\n",
    "        :param mask: Optional mask to apply at the end. Shape (batch x N x M)\n",
    "        :return: The output embeddings. Shape (batch x N x M x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == 4, \"Embeddings must have 4 dimensions (batch x N x M x embedding_dim)\"\n",
    "        assert embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {embeddings.shape[-1]}\"\n",
    "\n",
    "        # Permute to match the expected input shape for Conv2d\n",
    "        channels = embeddings.permute(0, 3, 1, 2)  # shape: (batch x embedding_dim x N x M)\n",
    "        channels = self.layers(channels)  # shape: (batch x embedding_dim x N x M)\n",
    "        channels = self.dropout(channels)  # Apply dropout here\n",
    "        embeddings = channels.permute(0, 2, 3, 1)  # shape: (batch x N x M x embedding_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.dim() == 3, \"Mask must have 3 dimensions (batch x N x M)\"\n",
    "            mask = mask.unsqueeze(-1)  # shape: (batch x N x M x 1)\n",
    "            embeddings = embeddings*mask\n",
    "            \n",
    "        return embeddings\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "759dbfed6fa72c08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:15.779994Z",
     "start_time": "2024-08-05T23:43:15.658948Z"
    }
   },
   "source": [
    "class TestPixelCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of PixelCell\n",
    "        self.embedding_dim = 64\n",
    "        self.num_layers = 3\n",
    "        self.kernel_size = 3\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 8, 8\n",
    "        self.pixel_cell = PixelCell(self.embedding_dim, self.num_layers, self.kernel_size, self.dropout_prob)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.height, self.width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.embedding_dim)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.pixel_cell(embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.pixel_cell(embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")\n",
    "\n",
    "    def test_dropout_effect(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass multiple times to check for dropout effect\n",
    "        outputs = [self.pixel_cell(embeddings) for _ in range(5)]\n",
    "        \n",
    "        # Asserting that the outputs are different due to dropout\n",
    "        different_outputs = any(not torch.equal(outputs[i], outputs[i + 1]) for i in range(len(outputs) - 1))\n",
    "        self.assertTrue(different_outputs, \"Dropout does not seem to have an effect\")\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.height, self.width)\n",
    "        \n",
    "        # Running the forward pass with mask. Disable dropout so we can compare results.\n",
    "        self.pixel_cell.eval()\n",
    "        output = self.pixel_cell(embeddings, mask)\n",
    "        unmasked = self.pixel_cell(embeddings)\n",
    "        self.pixel_cell.train()\n",
    "\n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.height, self.width, self.embedding_dim))\n",
    "        \n",
    "        # Check that masking was applied correctly (if mask is all ones, output should be unaffected)\n",
    "        self.assertTrue(torch.allclose(output,unmasked))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.077s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "ef3a3b6a4a2fc1fd",
   "metadata": {},
   "source": [
    "## TextCell\n",
    "\n",
    "Designed for processing a stream of text embeddings. I am not sure if I will use it\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* num_layers: The number of encoding cells to use.\n",
    "* embedding_dim: The width of each individual embedding.\n",
    "* num_heads: The number of transformer heads.\n",
    "* dim_feedforward: The size of the feedforward layer\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: A tensor of text embeddings. Shape (batch x N x Embeddings)\n",
    "* mask: A mask of active text embeddings. Shape (batch x N x Embedding)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* Embeddings: An output sequence of embeddings. Shape (batch x N x Embeddings)\n",
    "\n",
    "**Design**\n",
    "\n",
    "Basically, we use a sequence of transformer encoder layers to encode the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "id": "7738923381a9d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:15.811237Z",
     "start_time": "2024-08-05T23:43:15.779994Z"
    }
   },
   "source": [
    "class TextCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based text processing cell. Processes a stream of text embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_layers: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dim_feedforward: int,\n",
    "                 dropout_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the transformer-based text cell\n",
    "        :param num_layers: The number of encoding cells to use.\n",
    "        :param embedding_dim: The width of each individual embedding.\n",
    "        :param num_heads: The number of transformer heads.\n",
    "        :param dim_feedforward: The size of the feedforward layer.\n",
    "        :param dropout_prob: The probability of an element to be zeroed (default: 0.1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Construct transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to process text embeddings through transformer encoder layers.\n",
    "        :param embeddings: A tensor of text embeddings. Shape (batch x N x embedding_dim)\n",
    "        :param mask: A mask of active text embeddings. Shape (batch x N)\n",
    "        :return: An output sequence of embeddings. Shape (batch x N x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == 3, \"Embeddings must have 3 dimensions (batch x N x embedding_dim)\"\n",
    "        assert embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {embeddings.shape[-1]}\"\n",
    "\n",
    "        # Pass through transformer encoder with optional mask            \n",
    "        output = self.transformer_encoder(embeddings, src_key_padding_mask=mask)\n",
    "        \n",
    "        if mask is not None:\n",
    "            assert mask.dim() == 2, \"Mask must have 2 dimensions (batch x N)\"\n",
    "            \n",
    "            # Mask style of project is inverted compared to torch convention\n",
    "            mask = 1 - mask\n",
    "            \n",
    "            # Process masking\n",
    "            mask = mask.unsqueeze(-1) # shape (batch x N x 1)\n",
    "            output = output*mask\n",
    "\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9197dfb5b93a771f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.045626Z",
     "start_time": "2024-08-05T23:43:15.811237Z"
    }
   },
   "source": [
    "class TestTextCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of TextCell\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = 64\n",
    "        self.num_heads = 8\n",
    "        self.dim_feedforward = 256\n",
    "        self.dropout_prob = 0.1\n",
    "        self.batch_size = 2\n",
    "        self.seq_length = 10\n",
    "        self.text_cell = TextCell(self.num_layers, self.embedding_dim, self.num_heads, self.dim_feedforward, self.dropout_prob)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim, 2)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.text_cell(embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.text_cell(embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.seq_length, dtype=torch.float)\n",
    "        \n",
    "        # Running the forward pass with mask\n",
    "        self.text_cell.eval()\n",
    "        output = self.text_cell(embeddings, mask)\n",
    "        no_mask = self.text_cell(embeddings)\n",
    "        self.text_cell.train()\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        # Creating dummy data for embeddings and partial mask\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        mask = torch.zeros(self.batch_size, self.seq_length, dtype=torch.float)\n",
    "        mask[:, :self.seq_length//2] = False  # Zero out the first half of the sequence for the mask\n",
    "        \n",
    "        # Running the forward pass with partial mask\n",
    "        output = self.text_cell(embeddings, mask)\n",
    "        \n",
    "        # Since the mask is True (or 1) where tokens are valid and False (or 0) where they are not,\n",
    "        # The masked parts of the output should be unaffected.\n",
    "        # No straightforward way to check without knowing transformer internals or ground truth values.\n",
    "        # For simplicity, we'll just check the output shape here.\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 0.189s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "2338dd26ae28bb64",
   "metadata": {},
   "source": [
    "## GramEncoder\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need something to convert a sequence of embeddings into a reduced-dimensions gram embedding.\n",
    "* It is easier to process gram matrices back to embeddings when the matrix dimensions are small\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* embedding_dim: The embeddin dimensions.\n",
    "* num_heads: The number of encoding heads\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* Embeddings: The embeddings to process. (batch x ... x E)\n",
    "* Mask: The masked embeddings. (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* Gram Embedding: The gram embedding for the situation. (batch x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "Basically, we make heads as in a transformer, then gram encode the heads, then recombine the results and feedforward. This will involve significantly less needed parameters than directly processing the encoding. In specific, we:\n",
    "\n",
    "* Mask the embeddings.\n",
    "* flatten into 1d embeddings.\n",
    "* reshape the input embedding dim into num_heads.\n",
    "* create and flatten gram matrices for each head. \n",
    "* concatenate the heads together and run them through a linear projection back into embedding_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4873556974e9a710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.076889Z",
     "start_time": "2024-08-05T23:43:16.045626Z"
    }
   },
   "source": [
    "class GramEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes embeddings as gram encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 num_heads: int):\n",
    "        \"\"\"\n",
    "        :param embedding_dim: The dimension of the embeddings\n",
    "        :param num_heads: The number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.encode_dim = self.head_dim ** 2\n",
    "        \n",
    "        self.combine = nn.Linear(self.num_heads * self.encode_dim, self.embedding_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_gram_encodings(embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create gram encodings from embeddings. These are created from a mean.\n",
    "        :param embeddings: The embeddings. (batch x num_heads x L x head_dim)\n",
    "        :param mask: The mask. (batch x L)\n",
    "        :return: The encodings. (batch x num_heads x head_dim^2)\n",
    "        \"\"\"\n",
    "        # Apply mask\n",
    "        embeddings = embeddings * mask.unsqueeze(1).unsqueeze(-1)\n",
    "        \n",
    "        # Compute the gram matrix\n",
    "        gram_matrix = torch.matmul(embeddings.permute(0, 1, 3, 2), embeddings)    \n",
    "        encodings = gram_matrix.flatten(2, -1)  # shape (batch x num_heads x head_dim^2)\n",
    "        \n",
    "        # Normalize the gram encodings by the active entries that contributed. This takes the mean and handles variable sizes.\n",
    "        counts = mask.sum(dim=-1, keepdim=True)  # (batch x 1)\n",
    "        counts = counts.unsqueeze(1)  # shape: (batch x 1 x 1)\n",
    "        encodings = encodings / (counts + 1e-8)  # shape (batch x num_heads x head_dim^2)\n",
    "\n",
    "        return encodings\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward method to produce gram embeddings\n",
    "        :param embeddings: The input embeddings. Shape (batch_size x ... x E)\n",
    "        :param mask: The input mask. Shape (batch_size x ...)\n",
    "        :return: The gram embedding. Shape (batch_size x E)\n",
    "        \"\"\"\n",
    "        # Check input shapes\n",
    "        assert embeddings.dim() >= 3, \"Embeddings must have at least 3 dimensions (batch_size, ..., E)\"\n",
    "        assert mask.dim() == embeddings.dim() - 1, \"Mask must have one less dimension than embeddings\"\n",
    "\n",
    "        # Flatten\n",
    "        batch_size = embeddings.size(0)\n",
    "        embeddings = embeddings.flatten(1, -2)  # Shape(batch_size, L, E)\n",
    "        mask = mask.flatten(1, -1)  # Shape (batch x L)\n",
    "\n",
    "        # Create heads\n",
    "        heads = embeddings.view(batch_size, -1, self.num_heads, self.head_dim)  # Shape (batch x L x num_heads x head_dim)\n",
    "        heads = heads.permute(0, 2, 1, 3)  # Shape (batch x num_heads x L x head_dim)\n",
    "        # Process gram encodings\n",
    "        encodings = self.create_gram_encodings(heads, mask)  # (batch x num_heads x head_dim^2)\n",
    "\n",
    "        # Recombine\n",
    "        encodings = encodings.flatten(1, -1)  # shape: (batch x num_heads * head_dim^2)\n",
    "        embeddings = self.combine(encodings)  # shape: (batch x E)\n",
    "        return embeddings"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "fba11f68377bca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.280016Z",
     "start_time": "2024-08-05T23:43:16.076889Z"
    }
   },
   "source": [
    "class TestGramEncoder(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of GramEncoder\n",
    "        self.embedding_dim = 128\n",
    "        self.num_heads = 4\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 8, 8\n",
    "        self.encoder = GramEncoder(self.embedding_dim, self.num_heads)\n",
    "\n",
    "    def test_gram_encoding(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.height, self.width)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        gram_embedding = self.encoder(embeddings, mask)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(gram_embedding.shape, (self.batch_size, self.embedding_dim))\n",
    "        \n",
    "        # Additional tests can be added here to check specific values or properties\n",
    "        # Example: check if the output is not NaN\n",
    "        self.assertFalse(torch.isnan(gram_embedding).any(), \"Output contains NaNs\")\n",
    "        \n",
    "        # Example: check if the output is not infinite\n",
    "        self.assertFalse(torch.isinf(gram_embedding).any(), \"Output contains infinite values\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 15 tests in 0.179s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "c58e64b4b3b6c2df",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "The feedforward layer works a lot like a transformers. It lets the model decide to respond differently with different encoding inputs\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* embedding_dim: The dimension of the embedding input\n",
    "* feedforward_dim: The dimension of the internal feedforward channel\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "*gram_embeddings: Embeddings. Shape (batch x embeddings)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "*gram_embeddings: Embeddings. Shape (batch x embeddings)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We have a linear, a relu, and a linear. Not much else to say."
   ]
  },
  {
   "cell_type": "code",
   "id": "668a914724a22c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.295636Z",
     "start_time": "2024-08-05T23:43:16.280016Z"
    }
   },
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward layer. Works similarly to a transformer feedforward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, feedforward_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the feedforward layer\n",
    "        :param embedding_dim: The dimension of the embedding input.\n",
    "        :param feedforward_dim: The dimension of the internal feedforward channel.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "\n",
    "        # Define the feedforward network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feedforward_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, gram_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the feedforward network.\n",
    "        :param gram_embeddings: Embeddings. Shape (batch x embedding_dim)\n",
    "        :return: Embeddings. Shape (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert gram_embeddings.dim() == 2, \"gram_embeddings must have 2 dimensions (batch x embedding_dim)\"\n",
    "        assert gram_embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {gram_embeddings.shape[-1]}\"\n",
    "\n",
    "        return self.network(gram_embeddings)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "c6abb3a3a38283e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.530056Z",
     "start_time": "2024-08-05T23:43:16.295636Z"
    }
   },
   "source": [
    "class TestFeedforward(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of Feedforward\n",
    "        self.embedding_dim = 64\n",
    "        self.feedforward_dim = 256\n",
    "        self.batch_size = 32\n",
    "        self.feedforward_layer = FeedForward(self.embedding_dim, self.feedforward_dim)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim, 2)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.feedforward_layer(gram_embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.feedforward_layer(gram_embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 20 tests in 0.188s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "befe1e32086315a8",
   "metadata": {},
   "source": [
    "# Cell layers \n",
    "\n",
    "We specify the main BGC cell layer utilized for the encoding and decoding process, along with the sub layers and the builders. We test thoroughly along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5681aa1559230c6",
   "metadata": {},
   "source": [
    "## BGCEncoderCell\n",
    "\n",
    "An encoder cell that takes in an image embedding and produces a gram embedding. \n",
    "\n",
    "**Premise**\n",
    "\n",
    "We need to be able to encode an input into a gram embedding for this architecture to work.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* encoder: The encoding stack.\n",
    "* gram_encoder: The gram encoder\n",
    "* feedforward: A feedforward network that goes off against the gram embeddings\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: a batch of 2d positional embeddings: (batch x ... x E)\n",
    "* mask: a mask for the batch elements, True means active: (batch x ...)\n",
    "* pos_encoding: a grid of positional encodings to inject. Shape (batch x ... x E)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* gram_embedding: A gram embedding. Shape (batch x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "The layer starts by injecting the positional encodings into the input, then processes them using\n",
    "the encoder. The result is passed into the GramEncoder, which is then passed back out.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8192c797ad76451c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.545611Z",
     "start_time": "2024-08-05T23:43:16.530056Z"
    }
   },
   "source": [
    "class BGCEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder cell that takes in a collection of embeddings and produces a gram embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: nn.Module, gram_encoder: nn.Module, feedforward: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCEncoderCell\n",
    "        :param encoder: The encoding stack.\n",
    "        :param gram_encoder: The gram encoder.\n",
    "        :param feedforward: A feedforward network that goes off against the gram embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gram_encoder = gram_encoder\n",
    "        self.feedforward = feedforward\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor, pos_encoding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to produce a gram embedding from input embeddings.\n",
    "        :param embeddings: A batch of positional embeddings. Shape (batch x ... x embedding_dim)\n",
    "        :param mask: A mask for the batch elements, True means active. Shape (batch x ...)\n",
    "        :param pos_encoding: A grid of positional encodings to inject. Shape (batch x ... x embedding_dim)\n",
    "        :return: A gram embedding. Shape (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == pos_encoding.dim(), \"Embeddings and positional encodings must have the same number of dimensions\"\n",
    "        assert embeddings.shape[-1] == pos_encoding.shape[-1], \"Embeddings and positional encodings must have the same embedding dimension\"\n",
    "        assert embeddings.shape[:-1] == mask.shape, \"Embeddings and mask must have the same batch and spatial dimensions\"\n",
    "\n",
    "        # Inject positional encodings\n",
    "        embeddings = embeddings + pos_encoding\n",
    "\n",
    "        # Process with the encoder\n",
    "        encoded = self.encoder(embeddings, mask)\n",
    "\n",
    "        # Produce the gram embedding\n",
    "        gram_embedding = self.gram_encoder(encoded, mask)\n",
    "\n",
    "        # Pass the gram embedding through the feedforward network\n",
    "        gram_embedding = self.feedforward(gram_embedding)\n",
    "\n",
    "        return gram_embedding"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "fd2d778d536545d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.813357Z",
     "start_time": "2024-08-05T23:43:16.545611Z"
    }
   },
   "source": [
    "class TestBGCEncoderCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of BGCEncoderCell\n",
    "        self.embedding_dim = 64\n",
    "\n",
    "        # Dummy modules for encoder, gram_encoder, and feedforward\n",
    "        class DummyEncoder(nn.Module):\n",
    "            def forward(self, x, mask):\n",
    "                return x\n",
    "\n",
    "        class DummyGramEncoder(nn.Module):\n",
    "            def forward(self, x, mask):\n",
    "                return torch.mean(x, dim=tuple(range(1, x.dim() - 1)))\n",
    "\n",
    "        class DummyFeedforward(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return x\n",
    "\n",
    "        self.encoder = DummyEncoder()\n",
    "        self.gram_encoder = DummyGramEncoder()\n",
    "        self.feedforward = DummyFeedforward()\n",
    "\n",
    "        self.bgc_encoder_cell = BGCEncoderCell(self.encoder, self.gram_encoder, self.feedforward)\n",
    "\n",
    "    def test_forward_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_3d(self):\n",
    "        batch_size = 2\n",
    "        depth, height, width = 4, 8, 8\n",
    "        embeddings = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, depth, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim, 2)\n",
    "        mask = torch.ones(batch_size, seq_length, 2)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim, 2)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim + 1)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        mask[:, :seq_length // 2] = 0  # Zero out the first half of the sequence for the mask\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 27 tests in 0.225s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "7f048060e1e8dd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:16.844601Z",
     "start_time": "2024-08-05T23:43:16.813357Z"
    }
   },
   "source": [
    "def build_bgc_encoder_cell_for_text(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_feedforward: int,\n",
    "        dropout_prob: float,\n",
    "        gram_feedforward: int,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCEncoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCEncoderCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param transformer_heads: The number of transformer heads in the TextCell.\n",
    "    :param transformer_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :param gram_feedforward: The dimension of the internal feedforward network for the gram encoder.\n",
    "    :param gram_heads: The number of heads in the GramEncoder.\n",
    "    :return: An instance of BGCEncoderCell configured for text data.\n",
    "    \"\"\"\n",
    "    encoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=transformer_heads,\n",
    "        dim_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=gram_feedforward\n",
    "    )\n",
    "    return BGCEncoderCell(encoder, gram_encoder, feedforward)\n",
    "\n",
    "def build_bgc_encoder_cell_for_pixel(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        kernel_size: int,\n",
    "        dropout_prob: float,\n",
    "        gram_feedforward: int,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCEncoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCEncoderCell for pixel data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :param gram_feedforward: The dimension of the internal feedforward network for the gram encoder.\n",
    "    :param gram_heads: The number of heads in the GramEncoder.\n",
    "    :return: An instance of BGCEncoderCell configured for pixel data.\n",
    "    \"\"\"\n",
    "    encoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=gram_feedforward\n",
    "    )\n",
    "    return BGCEncoderCell(encoder, gram_encoder, feedforward)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "ef4bce6861d35351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.175363Z",
     "start_time": "2024-08-05T23:43:16.844601Z"
    }
   },
   "source": [
    "class TestBGCEncoderCellBuilders(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters for text and pixel BGCEncoderCell builders\n",
    "        self.num_layers_text = 4\n",
    "        self.embedding_dim_text = 64\n",
    "        self.transformer_heads_text = 8\n",
    "        self.transformer_feedforward_text = 256\n",
    "        self.dropout_prob_text = 0.1\n",
    "        self.gram_feedforward_text = 128\n",
    "        self.gram_heads_text = 4\n",
    "\n",
    "        self.num_layers_pixel = 3\n",
    "        self.embedding_dim_pixel = 64\n",
    "        self.kernel_size_pixel = 3\n",
    "        self.dropout_prob_pixel = 0.5\n",
    "        self.gram_feedforward_pixel = 128\n",
    "        self.gram_heads_pixel = 4\n",
    "\n",
    "    def test_build_bgc_encoder_cell_for_text(self):\n",
    "        text_bgc_encoder_cell = build_bgc_encoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text,\n",
    "            gram_feedforward=self.gram_feedforward_text,\n",
    "            gram_heads=self.gram_heads_text\n",
    "        )\n",
    "        self.assertIsInstance(text_bgc_encoder_cell, BGCEncoderCell)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.encoder, TextCell)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.gram_encoder, GramEncoder)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.feedforward, FeedForward)\n",
    "\n",
    "    def test_build_bgc_encoder_cell_for_pixel(self):\n",
    "        pixel_bgc_encoder_cell = build_bgc_encoder_cell_for_pixel(\n",
    "            num_layers=self.num_layers_pixel,\n",
    "            embedding_dim=self.embedding_dim_pixel,\n",
    "            kernel_size=self.kernel_size_pixel,\n",
    "            dropout_prob=self.dropout_prob_pixel,\n",
    "            gram_feedforward=self.gram_feedforward_pixel,\n",
    "            gram_heads=self.gram_heads_pixel\n",
    "        )\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell, BGCEncoderCell)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.encoder, PixelCell)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.gram_encoder, GramEncoder)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.feedforward, FeedForward)\n",
    "\n",
    "    def test_integration_text_bgc_encoder_cell(self):\n",
    "        text_bgc_encoder_cell = build_bgc_encoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text,\n",
    "            gram_feedforward=self.gram_feedforward_text,\n",
    "            gram_heads=self.gram_heads_text\n",
    "        )\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "\n",
    "        output = text_bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim_text))\n",
    "\n",
    "    def test_integration_pixel_bgc_encoder_cell(self):\n",
    "        pixel_bgc_encoder_cell = build_bgc_encoder_cell_for_pixel(\n",
    "            num_layers=self.num_layers_pixel,\n",
    "            embedding_dim=self.embedding_dim_pixel,\n",
    "            kernel_size=self.kernel_size_pixel,\n",
    "            dropout_prob=self.dropout_prob_pixel,\n",
    "            gram_feedforward=self.gram_feedforward_pixel,\n",
    "            gram_heads=self.gram_heads_pixel\n",
    "        )\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim_pixel)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim_pixel)\n",
    "\n",
    "        output = pixel_bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim_pixel))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_build_bgc_encoder_cell_for_pixel (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_build_bgc_encoder_cell_for_text (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_pixel_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 31 tests in 0.291s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "4e5b4cbe747bebb7",
   "metadata": {},
   "source": [
    "## BGCDecoderCell\n",
    "\n",
    "A decode cell turns a gram embedding into an update that can be applied.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "We need to be able to create updates from gram embeddings in order to encode and decode.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `decoder`: The primary decoder mechanism, likely a stack of convolutional networks or similar.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `gram_embedding`: The gram embedding of the layer. Shape: (batch x embedding_dim).\n",
    "* `mask`: The mask indicating active elements. Shape: (batch x ...).\n",
    "* `pos_encodings`: The positional encodings to use. Shape: (batch x ... x embedding_dim).\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `update`: The update built from the gram embeddings. Shape: (batch x ... x embedding_dim).\n",
    "\n",
    "**Design**\n",
    "\n",
    "We follow the following steps:\n",
    "\n",
    "* Expand the `gram_embedding` to match the shape of `pos_encodings`.\n",
    "* Add the positional encodings to the expanded gram embedding and multiply by the mask.\n",
    "* Run the combined input through the decoder layer.\n",
    "* Return the result as the update.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3937f2f5edeca302",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.190977Z",
     "start_time": "2024-08-05T23:43:17.175363Z"
    }
   },
   "source": [
    "class BGCDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A decode cell turns a gram embedding into an update that can be applied.\n",
    "    \"\"\"\n",
    "    def __init__(self, decoder: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCDecoderCell\n",
    "        :param decoder: The primary decoder mechanism. Likely a stack of convolutional networks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, gram_embedding: torch.Tensor, mask: torch.Tensor, pos_encodings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to produce an update from gram embeddings.\n",
    "        :param gram_embedding: The gram embedding of the layer. Shape (batch x embedding_dim).\n",
    "        :param mask: The mask indicating active elements. Shape (batch x ...).\n",
    "        :param pos_encodings: The positional encodings to use. Shape (batch x ... x embedding_dim).\n",
    "        :return: The update built from the gram embeddings. Shape: (batch x ... x embedding_dim).\n",
    "        \"\"\"\n",
    "        assert gram_embedding.dim() == 2, \"Gram embedding must have 2 dimensions (batch x embedding_dim)\"\n",
    "        assert pos_encodings.shape[:-1] == mask.shape, \"Positional encodings and mask must have the same shape except for the last dimension\"\n",
    "        assert pos_encodings.shape[-1] == gram_embedding.shape[-1], \"Positional encodings and gram embeddings must have the same embedding dimension\"\n",
    "\n",
    "        # Expand gram_embedding to match pos_encoding shape.\n",
    "        while gram_embedding.dim() < pos_encodings.dim():\n",
    "            gram_embedding = gram_embedding.unsqueeze(1)\n",
    "\n",
    "        # Take the positional encodings, add it to the expanded gram embedding, and multiply by the mask\n",
    "        combined_input = (gram_embedding + pos_encodings) * mask.unsqueeze(-1)\n",
    "\n",
    "        # Run the combined input through the decoder\n",
    "        update = self.decoder(combined_input)\n",
    "\n",
    "        return update"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "d4cf6278bec25ad9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.222218Z",
     "start_time": "2024-08-05T23:43:17.190977Z"
    }
   },
   "source": [
    "\n",
    "class DummyDecoder(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TestBGCDecoderCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of BGCDecoderCell\n",
    "        self.decoder = DummyDecoder()\n",
    "        self.bgc_decoder_cell = BGCDecoderCell(self.decoder)\n",
    "        self.embedding_dim = 64\n",
    "\n",
    "    def test_forward_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_3d(self):\n",
    "        batch_size = 2\n",
    "        depth, height, width = 4, 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, depth, height, width)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, depth, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim, 2)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim + 1)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        mask[:, :seq_length // 2] = 0  # Zero out the first half of the sequence for the mask\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "ed4af20c9147076a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.237845Z",
     "start_time": "2024-08-05T23:43:17.222218Z"
    }
   },
   "source": [
    "def build_bgc_decoder_cell_for_text(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_feedforward: int,\n",
    "        dropout_prob: float,\n",
    "        **kwargs\n",
    "    ) -> BGCDecoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCDecoderCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param transformer_heads: The number of transformer heads in the TextCell.\n",
    "    :param transformer_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :return: An instance of BGCDecoderCell configured for text data.\n",
    "    \"\"\"\n",
    "    decoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=transformer_heads,\n",
    "        dim_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    return BGCDecoderCell(decoder)\n",
    "\n",
    "def build_bgc_decoder_cell_for_image(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        kernel_size: int,\n",
    "        dropout_prob: float,\n",
    "        **kwargs\n",
    "    ) -> BGCDecoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCDecoderCell for image data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :return: An instance of BGCDecoderCell configured for image data.\n",
    "    \"\"\"\n",
    "    decoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    return BGCDecoderCell(decoder)\n"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "47112207df4e9b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.706614Z",
     "start_time": "2024-08-05T23:43:17.237845Z"
    }
   },
   "source": [
    "class TestBGCDecoderCellBuilders(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters for text and image BGCDecoderCell builders\n",
    "        self.num_layers_text = 4\n",
    "        self.embedding_dim_text = 64\n",
    "        self.transformer_heads_text = 8\n",
    "        self.transformer_feedforward_text = 256\n",
    "        self.dropout_prob_text = 0.1\n",
    "\n",
    "        self.num_layers_image = 3\n",
    "        self.embedding_dim_image = 64\n",
    "        self.kernel_size_image = 3\n",
    "        self.dropout_prob_image = 0.5\n",
    "\n",
    "    def test_build_bgc_decoder_cell_for_text(self):\n",
    "        text_bgc_decoder_cell = build_bgc_decoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text\n",
    "        )\n",
    "        self.assertIsInstance(text_bgc_decoder_cell, BGCDecoderCell)\n",
    "        self.assertIsInstance(text_bgc_decoder_cell.decoder, TextCell)\n",
    "\n",
    "    def test_build_bgc_decoder_cell_for_image(self):\n",
    "        image_bgc_decoder_cell = build_bgc_decoder_cell_for_image(\n",
    "            num_layers=self.num_layers_image,\n",
    "            embedding_dim=self.embedding_dim_image,\n",
    "            kernel_size=self.kernel_size_image,\n",
    "            dropout_prob=self.dropout_prob_image\n",
    "        )\n",
    "        self.assertIsInstance(image_bgc_decoder_cell, BGCDecoderCell)\n",
    "        self.assertIsInstance(image_bgc_decoder_cell.decoder, PixelCell)\n",
    "\n",
    "    def test_integration_text_bgc_decoder_cell(self):\n",
    "        text_bgc_decoder_cell = build_bgc_decoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text\n",
    "        )\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim_text)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = text_bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim_text))\n",
    "\n",
    "    def test_integration_image_bgc_decoder_cell(self):\n",
    "        image_bgc_decoder_cell = build_bgc_decoder_cell_for_image(\n",
    "            num_layers=self.num_layers_image,\n",
    "            embedding_dim=self.embedding_dim_image,\n",
    "            kernel_size=self.kernel_size_image,\n",
    "            dropout_prob=self.dropout_prob_image\n",
    "        )\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim_image)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim_image)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        output = image_bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, height, width, self.embedding_dim_image))\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_forward_invalid_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_build_bgc_decoder_cell_for_image (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_build_bgc_decoder_cell_for_text (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_image_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_build_bgc_encoder_cell_for_pixel (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_build_bgc_encoder_cell_for_text (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_pixel_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 42 tests in 0.437s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "ccd80a79224cd291",
   "metadata": {},
   "source": [
    "## BGCCell\n",
    "\n",
    "A decoder/encoder layer capable of performing the decoding or encoding action on demand.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need to specify a cell to actually do the BGC process.\n",
    "* It can elegantly support bidirectionality if we subtract when decoding and add when encoding.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `encoder_cell` [BGCEncoderCell]: Encodes an input into gram embeddings.\n",
    "* `decoder_cell` [BCGDecoderCell]: Starts from a gram embedding and the shape, and produces an update.\n",
    "\n",
    "### Method: Encode\n",
    "\n",
    "This is the encode action for the cell.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: A batch of embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `pos_encodings`: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "* `mask`: A mask indicating active elements. Shape: (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `gram_embeddings`: The gram embedding for the layer. Shape: (batch x embedding_dim)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We follow the following sequence of events:\n",
    "\n",
    "* Use the `encoder_cell` to create a `gram_embedding` from the inputs.\n",
    "* Use the `decoder_cell` to create an 'update' from the `gram_embedding`.\n",
    "* Subtract the 'update' from the original embeddings.\n",
    "* Return the embeddings and `gram_embedding`.\n",
    "\n",
    "### Method: Decode\n",
    "\n",
    "Runs a decode action for the layer. Conceptually, this is like the encode action but in reverse.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: The current embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `pos_encodings`: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "* `gram_encoding`: The gram encoding for the layer. Shape: (batch x embedding_dim)\n",
    "* `mask`: The mask for the layer. Shape: (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We proceed similarly to encoding, except we use the decoder cell, and we add the update instead:\n",
    "\n",
    "* Use the `decoder_cell` to create an 'update' from the `gram_encoding`.\n",
    "* Add the update to the embeddings.\n",
    "* Return the resulting embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6747d035ca0fb0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:17.737848Z",
     "start_time": "2024-08-05T23:43:17.706614Z"
    }
   },
   "source": [
    "class BGCCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder/encoder layer capable of performing the decoding or encoding action on demand.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_cell: nn.Module, decoder_cell: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCCell\n",
    "        :param encoder_cell: Encodes an input into gram embeddings.\n",
    "        :param decoder_cell: Starts from a gram embedding and the shape, and produces an update.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_cell = encoder_cell\n",
    "        self.decoder_cell = decoder_cell\n",
    "\n",
    "    def encode(self, embeddings: torch.Tensor, pos_encodings: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode action for the cell.\n",
    "        :param embeddings: A batch of embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        :param pos_encodings: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "        :param mask: A mask indicating active elements. Shape: (batch x ...)\n",
    "        :return: embeddings: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "                 gram_embeddings: The gram embedding for the layer. Shape: (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        # Use the encoder_cell to create a gram_embedding from the inputs\n",
    "        gram_embedding = self.encoder_cell(embeddings, mask, pos_encodings)\n",
    "\n",
    "        # Use the decoder_cell to create an 'update' from the gram_embedding\n",
    "        update = self.decoder_cell(gram_embedding, mask, pos_encodings)\n",
    "\n",
    "        # Subtract the 'update' from the original embeddings\n",
    "        embeddings = embeddings - update\n",
    "\n",
    "        return embeddings, gram_embedding\n",
    "\n",
    "    def decode(self, embeddings: torch.Tensor, pos_encodings: torch.Tensor, gram_encoding: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode action for the cell.\n",
    "        :param embeddings: The current embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        :param pos_encodings: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "        :param gram_encoding: The gram encoding for the layer. Shape: (batch x embedding_dim)\n",
    "        :param mask: The mask for the layer. Shape: (batch x ...)\n",
    "        :return: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        \"\"\"\n",
    "        # Use the decoder_cell to create an 'update' from the gram_encoding\n",
    "        update = self.decoder_cell(gram_encoding, mask, pos_encodings)\n",
    "\n",
    "        # Add the update to the embeddings\n",
    "        embeddings = embeddings + update\n",
    "\n",
    "        return embeddings"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "82d01e8859841475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:18.151216Z",
     "start_time": "2024-08-05T23:43:17.737848Z"
    }
   },
   "source": [
    "class DummyEncoderCell(nn.Module):\n",
    "    def forward(self, embeddings, mask, pos_encodings):\n",
    "        # Flatten everything between the first and last dimension\n",
    "        flattened_embeddings = embeddings.flatten(start_dim=1, end_dim=-2)\n",
    "        # Take the mean of the middle dimension\n",
    "        return torch.mean(flattened_embeddings, dim=1)  # Simplified\n",
    "\n",
    "class DummyDecoderCell(nn.Module):\n",
    "    def forward(self, gram_embedding, mask, pos_encodings):\n",
    "        # Just return the pos_encodings as it has the right shape\n",
    "        return pos_encodings\n",
    "\n",
    "class TestBGCCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embedding_dim = 64\n",
    "        self.encoder_cell = DummyEncoderCell()\n",
    "        self.decoder_cell = DummyDecoderCell()\n",
    "        self.bgc_cell = BGCCell(self.encoder_cell, self.decoder_cell)\n",
    "\n",
    "    def test_encode_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        self.assertEqual(encoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "        self.assertEqual(gram_embedding.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_encode_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        self.assertEqual(encoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n",
    "        self.assertEqual(gram_embedding.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_decode_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        gram_encoding = torch.randn(batch_size, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        decoded_embeddings = self.bgc_cell.decode(embeddings, pos_encodings, gram_encoding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_decode_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        gram_encoding = torch.randn(batch_size, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        decoded_embeddings = self.bgc_cell.decode(embeddings, pos_encodings, gram_encoding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_encode_decode_consistency_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "\n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        decoded_embeddings = self.bgc_cell.decode(encoded_embeddings, pos_encodings, gram_embedding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_encode_decode_consistency_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "\n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        decoded_embeddings = self.bgc_cell.decode(encoded_embeddings, pos_encodings, gram_embedding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_decode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_decode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_build_bgc_decoder_cell_for_image (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_build_bgc_decoder_cell_for_text (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_image_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_build_bgc_encoder_cell_for_pixel (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_build_bgc_encoder_cell_for_text (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_pixel_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 48 tests in 0.366s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "366aef0a41d44add",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:18.182454Z",
     "start_time": "2024-08-05T23:43:18.151216Z"
    }
   },
   "source": [
    "def build_bgc_cell_for_text(\n",
    "    num_layers: int,\n",
    "    embedding_dim: int,\n",
    "    transformer_heads: int,\n",
    "    transformer_feedforward: int,\n",
    "    dropout_prob: float,\n",
    "    gram_heads: int,\n",
    "    gram_feedforward: int,\n",
    "    **kwargs\n",
    ") -> BGCCell:\n",
    "    \"\"\"\n",
    "    Build a BGCCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param transformer_heads: The number of transformer heads in the TextCell.\n",
    "    :param transformer_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :param gram_heads: The number of heads for the GramEncoder.\n",
    "    :param gram_feedforward: The size of the feedforward layer in the GramEncoder.\n",
    "    :return: An instance of BGCCell configured for text data.\n",
    "    \"\"\"\n",
    "    # Build encoder cell\n",
    "    encoder_cell = build_bgc_encoder_cell_for_text(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        transformer_heads=transformer_heads,\n",
    "        transformer_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob,\n",
    "        gram_feedforward=gram_feedforward,\n",
    "        gram_heads=gram_heads,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Build decoder cell\n",
    "    decoder_cell = build_bgc_decoder_cell_for_text(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        transformer_heads=transformer_heads,\n",
    "        transformer_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Return BGCCell\n",
    "    return BGCCell(encoder_cell, decoder_cell)\n",
    "\n",
    "\n",
    "def build_bgc_cell_for_image(\n",
    "    num_layers: int,\n",
    "    embedding_dim: int,\n",
    "    kernel_size: int,\n",
    "    dropout_prob: float,\n",
    "    gram_heads: int,\n",
    "    gram_feedforward: int,\n",
    "    **kwargs\n",
    ") -> BGCCell:\n",
    "    \"\"\"\n",
    "    Build a BGCCell for image data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :param gram_heads: The number of heads for the GramEncoder.\n",
    "    :param gram_feedforward: The size of the feedforward layer in the GramEncoder.\n",
    "    :return: An instance of BGCCell configured for image data.\n",
    "    \"\"\"\n",
    "    # Build encoder cell\n",
    "    encoder_cell = build_bgc_encoder_cell_for_pixel(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob,\n",
    "        gram_feedforward=gram_feedforward,\n",
    "        gram_heads=gram_heads,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Build decoder cell\n",
    "    decoder_cell = build_bgc_decoder_cell_for_image(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    # Return BGCCell\n",
    "    return BGCCell(encoder_cell, decoder_cell)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Main Architecture\n",
    "\n",
    "We define the main portions of the architecture, including top level encoding and decoding"
   ],
   "id": "1f39abb0c69bff28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Positional encodings computation functions\n",
    "\n",
    "These are functions that are capable of producing positional encodings."
   ],
   "id": "187ec9427479b659"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:18.213728Z",
     "start_time": "2024-08-05T23:43:18.182454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sinusoidal_positional_encoding(seq_len, model_dim, gen_term=10000.0):\n",
    "    \"\"\"\n",
    "    Computes sinusoidal positional encoding for a transformer.\n",
    "\n",
    "    Parameters:\n",
    "    seq_len (int): Length of the sequence.\n",
    "    model_dim (int): Dimension of the model (d_model).\n",
    "    gen_term (float): Generation term for frequency calculation (default: 10000.0).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Positional encoding matrix of shape (seq_len, model_dim).\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, model_dim, 2) * -(np.log(gen_term) / model_dim))\n",
    "    pos_enc = np.zeros((seq_len, model_dim))\n",
    "    pos_enc[:, 0::2] = np.sin(position * div_term)\n",
    "    pos_enc[:, 1::2] = np.cos(position * div_term)\n",
    "    return pos_enc\n",
    "\n",
    "\n",
    "def eval_legendre(order, x):\n",
    "    \"\"\"\n",
    "    Evaluates the Legendre polynomial of a given order at points x.\n",
    "\n",
    "    Parameters:\n",
    "    order (int): The order of the Legendre polynomial.\n",
    "    x (numpy.ndarray): The points at which to evaluate the polynomial.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The evaluated Legendre polynomial values.\n",
    "    \"\"\"\n",
    "    if order == 0:\n",
    "        return np.ones_like(x)\n",
    "    elif order == 1:\n",
    "        return x\n",
    "    else:\n",
    "        p0 = np.ones_like(x)\n",
    "        p1 = x\n",
    "        for n in range(2, order + 1):\n",
    "            pn = ((2 * n - 1) * x * p1 - (n - 1) * p0) / n\n",
    "            p0, p1 = p1, pn\n",
    "        return pn\n",
    "\n",
    "def pope_positional_encoding(seq_len, model_dim, gen_term=10):\n",
    "    \"\"\"\n",
    "    Computes PoPE positional encoding using Legendre polynomials.\n",
    "\n",
    "    Parameters:\n",
    "    seq_len (int): Length of the sequence.\n",
    "    model_dim (int): Dimension of the model (d_model).\n",
    "    order (int): Order of the Legendre polynomial (default: 10).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: PoPE encoding matrix of shape (seq_len, model_dim).\n",
    "    \"\"\"\n",
    "    position = np.linspace(-1, 1, seq_len)\n",
    "    pos_enc = np.zeros((seq_len, model_dim))\n",
    "    for i in range(model_dim):\n",
    "        pos_enc[:, i] = eval_legendre(gen_term, position) * ((i + 1) / model_dim)\n",
    "    return pos_enc"
   ],
   "id": "6ca9cc0a556210e6",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T23:43:18.780942Z",
     "start_time": "2024-08-05T23:43:18.213728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TestPositionalEncodings(unittest.TestCase):\n",
    "\n",
    "    def test_sinusoidal_positional_encoding_shape(self):\n",
    "        seq_len = 50\n",
    "        model_dim = 512\n",
    "        encoding = sinusoidal_positional_encoding(seq_len, model_dim, gen_term=10000.0)\n",
    "        self.assertEqual(encoding.shape, (seq_len, model_dim))\n",
    "\n",
    "    def test_sinusoidal_positional_encoding_values(self):\n",
    "        seq_len = 2\n",
    "        model_dim = 4\n",
    "        gen_term = 10000.0\n",
    "        \n",
    "        \n",
    "        expected_first_term= np.array([0.0, 1.0, 0.0, 1.0])\n",
    "        encoding = sinusoidal_positional_encoding(seq_len, model_dim, gen_term)\n",
    "        np.testing.assert_almost_equal(encoding[0], expected_first_term, decimal=5)\n",
    "\n",
    "    def test_pope_positional_encoding_shape(self):\n",
    "        seq_len = 50\n",
    "        model_dim = 512\n",
    "        encoding = pope_positional_encoding(seq_len, model_dim, gen_term=10)\n",
    "        self.assertEqual(encoding.shape, (seq_len, model_dim))\n",
    "\n",
    "    def test_pope_positional_encoding_values(self):\n",
    "        seq_len = 2\n",
    "        model_dim = 4\n",
    "        order = 10\n",
    "        position = np.linspace(-1, 1, seq_len)\n",
    "        expected = np.zeros((seq_len, model_dim))\n",
    "        for i in range(model_dim):\n",
    "            expected[:, i] = eval_legendre(order, position) * ((i + 1) / model_dim)\n",
    "        encoding = pope_positional_encoding(seq_len, model_dim, order)\n",
    "        np.testing.assert_almost_equal(encoding, expected, decimal=5)\n",
    "\n",
    "    def test_pope_positional_encoding_non_zero(self):\n",
    "        seq_len = 50\n",
    "        model_dim = 512\n",
    "        encoding = pope_positional_encoding(seq_len, model_dim, gen_term=10)\n",
    "        self.assertTrue(np.any(encoding != 0), \"Positional encoding should not be all zeros.\")\n",
    "\n",
    "    def test_eval_legendre(self):\n",
    "        # Test known values of Legendre polynomials\n",
    "        x_values = np.array([-1, 0, 1])\n",
    "        # P_0(x) = 1\n",
    "        expected_p0 = np.array([1, 1, 1])\n",
    "        np.testing.assert_almost_equal(eval_legendre(0, x_values), expected_p0, decimal=5)\n",
    "\n",
    "        # P_1(x) = x\n",
    "        expected_p1 = np.array([-1, 0, 1])\n",
    "        np.testing.assert_almost_equal(eval_legendre(1, x_values), expected_p1, decimal=5)\n",
    "\n",
    "        # P_2(x) = 0.5 * (3x^2 - 1)\n",
    "        expected_p2 = np.array([1, -0.5, 1])\n",
    "        np.testing.assert_almost_equal(eval_legendre(2, x_values), expected_p2, decimal=5)\n",
    "\n",
    "        # P_3(x) = 0.5 * (5x^3 - 3x)\n",
    "        expected_p3 = np.array([-1, 0, 1])\n",
    "        np.testing.assert_almost_equal(eval_legendre(3, x_values), expected_p3, decimal=5)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)\n"
   ],
   "id": "c219e02d7856cfbb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_decode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_decode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_build_bgc_decoder_cell_for_image (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_build_bgc_decoder_cell_for_text (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_image_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_build_bgc_encoder_cell_for_pixel (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_build_bgc_encoder_cell_for_text (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_pixel_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_eval_legendre (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_non_zero (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_shape (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_values (__main__.TestPositionalEncodings) ... ok\n",
      "test_sinusoidal_positional_encoding_shape (__main__.TestPositionalEncodings) ... ok\n",
      "test_sinusoidal_positional_encoding_values (__main__.TestPositionalEncodings) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 54 tests in 0.525s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## PositionalEncodings\n",
    "\n",
    "The `PositionalEncodings` class produces multi-dimensional positional encodings that capture both the position and size of the input space within a single encoding. It precomputes these encodings to optimize runtime efficiency.\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "- `embedding_dim` [int]: The dimensionality of the output embedding.\n",
    "- `max_dim_sizes` [int | List[int]]: The maximum size expected per dimension. This is used to infer the shape for positional encoding.\n",
    "- `gen_function`: A function that generates positional encodings, accepting `sequence_length`, `embedding_dim`, and `gen_term`.\n",
    "- `gen_term`: A term used when generating encodings, serving as a conditioning parameter.\n",
    "\n",
    "### Accepts\n",
    "\n",
    "- `mask`: A mask indicating the active shape of the encodings, with shape `(batch x ...)`. It specifies the active terms.\n",
    "\n",
    "### Returns\n",
    "\n",
    "- `pos_encodings`: Positional encodings with shape `(batch x ... x embedding_dim)`, already masked and ready for further processing.\n",
    "\n",
    "### Design\n",
    "\n",
    "The design addresses two main challenges:\n",
    "\n",
    "1. **Handling Multi-dimensional `max_dim_sizes`**: The design accommodates both 1D and 2D inputs by checking the type and converting `max_dim_sizes` into a list if necessary. This flexibility allows the layer to adapt to various input configurations.\n",
    "\n",
    "2. **Encoding Grid Size and Position**: To capture both position and grid size, the layer:\n",
    "   - Precomputes a tensor that covers all dimensions of `max_dim_sizes`. This tensor is composed of concatenations of precomputations for each dimension, effectively encoding position and grid size together.\n",
    "   - Uses mesh grids to calculate positional encodings for each dimension.\n",
    "   - Concatenates these encodings using tensor products, doubling the number of dimensions to include both position and grid size.\n",
    "\n",
    "During the forward pass, the layer extracts relevant positional encodings and computes size encodings based on the mask. It then combines these encodings to produce the final output, reflecting the shape and position of the input data.\n"
   ],
   "id": "aada8e696c999f65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T00:03:48.516591Z",
     "start_time": "2024-08-06T00:03:48.484333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncodings(nn.Module):\n",
    "    \"\"\"\n",
    "    Produces N-D positional encodings to encode both position and size of input \n",
    "    space into a single encoding. Precomputes the encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 max_dim_sizes: int | List[int],\n",
    "                 gen_function: Callable[[int, int, Any], np.ndarray],\n",
    "                 gen_term: Any,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize and precompute \n",
    "        :param embedding_dim: The dimension of the embeddings to make\n",
    "        :param max_dim_sizes: A list indicating the maximum sizes the dimensions are going to see\n",
    "        :param gen_function: The function to generate the encodings\n",
    "        :param gen_term: The term conditioning encoding generation\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if isinstance(max_dim_sizes, int):\n",
    "            max_dim_sizes = [max_dim_sizes]\n",
    "            \n",
    "        # validate\n",
    "        total_split_factor = 2 * len(max_dim_sizes)\n",
    "        assert embedding_dim % total_split_factor == 0, f\"Embedding dim was not divisible by {total_split_factor}\"\n",
    "        \n",
    "        # Store\n",
    "        self.num_dims = len(max_dim_sizes)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.intermediate_dim = embedding_dim // total_split_factor\n",
    "        self.max_dim_sizes = max_dim_sizes\n",
    "        self.gen_func = gen_function\n",
    "        self.gen_term = gen_term\n",
    "        \n",
    "        # Pregenerate grids.\n",
    "        indices = [torch.arange(length) for length in max_dim_sizes]\n",
    "        index_vectors = torch.meshgrid(*indices, indexing=\"ij\")\n",
    "        dimension_precomputations = [gen_function(length, self.intermediate_dim, gen_term) for length in max_dim_sizes]\n",
    "        precomputed_encodings = [precomputed[index] for precomputed, index in zip(dimension_precomputations, index_vectors)]\n",
    "        precomputed_encodings = torch.concat([torch.tensor(precomputed) for precomputed in precomputed_encodings], dim=-1)\n",
    "        self.precomputed_encodings = precomputed_encodings\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_shape_size(mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the maximum size of each dimension per batch.\n",
    "        :param mask: The mask we were passed in. Shape (batch ...)\n",
    "        :return: The sizes tensor. Shape (batch x num_dims).\n",
    "        \"\"\"\n",
    "        \n",
    "        # We figure this out by assuming that the mask will consist of 1's right up to when we \n",
    "        # need to stop including new elements, at which point it will become zero. This means summing\n",
    "        # up along a given dimension will cause each element of the mask to display the number of \n",
    "        # elements on that dimension. \n",
    "        sums_per_dimension = [mask.sum(dim=dim, keepdim=True) for dim in range(1, len(mask.shape))]        \n",
    "        sums_per_dimension = [item.flatten(1, -1) for item in sums_per_dimension]\n",
    "        dim_sizes = torch.stack([item.max(dim=-1)[0].int() for item in sums_per_dimension ], dim=-1)\n",
    "        return dim_sizes\n",
    "        \n",
    "\n",
    "    def forward(self, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward sweep.\n",
    "        \n",
    "        :param mask: A floating mask of shape (batch x ...). Note that ... must match the number of dimensions.\n",
    "        :return: The computed and combined positional encodings.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert mask.dim() - 1 == self.num_dims, f\"The number of nonmask batch dimensions and positional dimensions were different\"\n",
    "        \n",
    "        # Start the positional encoding by slicing and expanding\n",
    "        \n",
    "        slicer = tuple([slice(dim) for dim in mask.shape[1:]] + [slice(None)])\n",
    "        pos_encoding = self.precomputed_encodings[slicer] #Shape (..., L)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).expand(mask.shape[0], *[-1]*pos_encoding.dim()) # Shape (batch x ... x L)\n",
    "        \n",
    "        # compute the size encoding.\n",
    "        sizes = self.compute_shape_size(mask) # batch x D\n",
    "        size_encoding = []\n",
    "        for index in sizes.unbind(0):\n",
    "            slicer = tuple([item for item in index] + [slice(None)])\n",
    "            size_encoding.append(self.precomputed_encodings[slicer])\n",
    "        size_encoding = torch.stack(size_encoding, dim = 0)\n",
    "        \n",
    "        while size_encoding.dim() < pos_encoding.dim():\n",
    "            size_encoding = size_encoding.unsqueeze(1)\n",
    "        size_encoding = size_encoding.expand(-1, *pos_encoding.shape[1:-1], -1)\n",
    "        \n",
    "        # Combine and return\n",
    "        pos_encoding = torch.cat([pos_encoding, size_encoding], dim=-1)\n",
    "        pos_encoding = pos_encoding*mask.unsqueeze(-1)\n",
    "        return pos_encoding\n"
   ],
   "id": "ec65a26984d3789b",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T00:04:22.284659Z",
     "start_time": "2024-08-06T00:04:21.544794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TestComputeSizes(unittest.TestCase):\n",
    "    def test_compute_sizes(self):\n",
    "        \n",
    "        # Define mask\n",
    "        mask = torch.zeros(2, 5, 5, dtype=torch.bool) \n",
    "        mask[0, :3, :4] = True\n",
    "        mask[1, :2, :3] = True\n",
    "        \n",
    "        # Define shapes\n",
    "        expected_shapes = [(3, 4), (2, 3)]\n",
    "        \n",
    "        # Compute and compare\n",
    "        outcome = PositionalEncodings.compute_shape_size(mask)\n",
    "        for actual, expected in zip(outcome.unbind(0), expected_shapes):\n",
    "            actual = (actual[0], actual[1])\n",
    "            self.assertEqual(actual, expected)    \n",
    "    \n",
    "    \n",
    "class TestPositionalEncoding(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embedding_dim = 4\n",
    "        self.gen_term = 100\n",
    "        self.gen_function = sinusoidal_positional_encoding\n",
    "    \n",
    "    \n",
    "    def test_1d_pos_encoding_constructor(self):\n",
    "        layer = PositionalEncodings(self.embedding_dim, 20, self.gen_function, self.gen_term)\n",
    "\n",
    "    def test_2d_pos_encoding_constructor(self):\n",
    "        layer = PositionalEncodings(self.embedding_dim, [5, 20], self.gen_function, self.gen_term)\n",
    "    \n",
    "    def test_1d_encoding(self):\n",
    "        \n",
    "        # Make layer\n",
    "        max_sequence_length = 14\n",
    "        layer = PositionalEncodings(self.embedding_dim, max_sequence_length, self.gen_function, self.gen_term)\n",
    "        \n",
    "        # Define mask\n",
    "        mask = torch.zeros(2, 5, dtype=torch.bool) \n",
    "        mask[0, :3] = True\n",
    "        mask[1, :2] = True        \n",
    "        \n",
    "        # Apply encoding\n",
    "        output = layer(mask)\n",
    "\n",
    "    def test_2d_encoding(self):\n",
    "        \n",
    "        # make layer\n",
    "        max_seq_length = [20, 15]\n",
    "        layer = PositionalEncodings(self.embedding_dim, max_seq_length, self.gen_function, self.gen_term)\n",
    "        \n",
    "        # Define mask\n",
    "        \n",
    "        mask = torch.zeros(2, 5, 10, dtype=torch.bool) \n",
    "        mask[0, :3, :4] = True\n",
    "        mask[1, :2, :7] = True        \n",
    "        \n",
    "        # Make encoding\n",
    "        output = layer(mask)\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=3, exit=False)"
   ],
   "id": "968eb8200a2a2191",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_decode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_decode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_decode_consistency_2d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_1d (__main__.TestBGCCell) ... ok\n",
      "test_encode_shape_2d (__main__.TestBGCCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCDecoderCell) ... ok\n",
      "test_build_bgc_decoder_cell_for_image (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_build_bgc_decoder_cell_for_text (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_image_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_decoder_cell (__main__.TestBGCDecoderCellBuilders) ... ok\n",
      "test_forward_invalid_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_1d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_2d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_shape_3d (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestBGCEncoderCell) ... ok\n",
      "test_build_bgc_encoder_cell_for_pixel (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_build_bgc_encoder_cell_for_text (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_pixel_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_integration_text_bgc_encoder_cell (__main__.TestBGCEncoderCellBuilders) ... ok\n",
      "test_compute_sizes (__main__.TestComputeSizes) ... ok\n",
      "test_forward_invalid_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestFeedforward) ... ok\n",
      "test_forward_shape (__main__.TestFeedforward) ... ok\n",
      "test_no_inf_values (__main__.TestFeedforward) ... ok\n",
      "test_no_nan_values (__main__.TestFeedforward) ... ok\n",
      "test_gram_encoding (__main__.TestGramEncoder) ... ok\n",
      "test_dropout_effect (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestPixelCell) ... ok\n",
      "test_forward_shape (__main__.TestPixelCell) ... ok\n",
      "test_forward_with_mask (__main__.TestPixelCell) ... ok\n",
      "test_no_inf_values (__main__.TestPixelCell) ... ok\n",
      "test_no_nan_values (__main__.TestPixelCell) ... ok\n",
      "test_1d_encoding (__main__.TestPositionalEncoding) ... ok\n",
      "test_1d_pos_encoding_constructor (__main__.TestPositionalEncoding) ... ok\n",
      "test_2d_encoding (__main__.TestPositionalEncoding) ... ok\n",
      "test_2d_pos_encoding_constructor (__main__.TestPositionalEncoding) ... ok\n",
      "test_eval_legendre (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_non_zero (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_shape (__main__.TestPositionalEncodings) ... ok\n",
      "test_pope_positional_encoding_values (__main__.TestPositionalEncodings) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.1411, -0.7568],\n",
      "          [ 0.0000,  0.8415,  0.1411, -0.7568],\n",
      "          [ 0.0000,  0.9093,  0.1411, -0.7568],\n",
      "          [ 0.0000,  0.1411,  0.1411, -0.7568],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.8415,  0.0000,  0.1411, -0.7568],\n",
      "          [ 0.8415,  0.8415,  0.1411, -0.7568],\n",
      "          [ 0.8415,  0.9093,  0.1411, -0.7568],\n",
      "          [ 0.8415,  0.1411,  0.1411, -0.7568],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.9093,  0.0000,  0.1411, -0.7568],\n",
      "          [ 0.9093,  0.8415,  0.1411, -0.7568],\n",
      "          [ 0.9093,  0.9093,  0.1411, -0.7568],\n",
      "          [ 0.9093,  0.1411,  0.1411, -0.7568],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000, -0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.9093,  0.6570],\n",
      "          [ 0.0000,  0.8415,  0.9093,  0.6570],\n",
      "          [ 0.0000,  0.9093,  0.9093,  0.6570],\n",
      "          [ 0.0000,  0.1411,  0.9093,  0.6570],\n",
      "          [ 0.0000, -0.7568,  0.9093,  0.6570],\n",
      "          [ 0.0000, -0.9589,  0.9093,  0.6570],\n",
      "          [ 0.0000, -0.2794,  0.9093,  0.6570],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.8415,  0.0000,  0.9093,  0.6570],\n",
      "          [ 0.8415,  0.8415,  0.9093,  0.6570],\n",
      "          [ 0.8415,  0.9093,  0.9093,  0.6570],\n",
      "          [ 0.8415,  0.1411,  0.9093,  0.6570],\n",
      "          [ 0.8415, -0.7568,  0.9093,  0.6570],\n",
      "          [ 0.8415, -0.9589,  0.9093,  0.6570],\n",
      "          [ 0.8415, -0.2794,  0.9093,  0.6570],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000,  0.0000,  0.0000,  0.0000]]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_sinusoidal_positional_encoding_shape (__main__.TestPositionalEncodings) ... ok\n",
      "test_sinusoidal_positional_encoding_values (__main__.TestPositionalEncodings) ... ok\n",
      "test_forward_invalid_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_invalid_embedding_dim (__main__.TestTextCell) ... ok\n",
      "test_forward_shape (__main__.TestTextCell) ... ok\n",
      "test_forward_with_mask (__main__.TestTextCell) ... ok\n",
      "test_forward_with_partial_mask (__main__.TestTextCell) ... ok\n",
      "test_no_inf_values (__main__.TestTextCell) ... ok\n",
      "test_no_nan_values (__main__.TestTextCell) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 59 tests in 0.704s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "id": "cb376a1241f0c9cb",
   "metadata": {},
   "source": [
    "## BidirectionalGramConverter\n",
    "\n",
    "The main converter model. It can convert into latent space, or from latent space back into an image grid. It has an encode mode, and a decode mode. \n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We can use the fact addition and subtraction are inverses to encode and decode respectively\n",
    "* gram encodings make good latent representations for this task\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* pos_encodings: The positional encoding mechanism. \n",
    "* cells List[BGCCell]: a list of BGC cells forming the foundational pieces of the model\n",
    "* encoding_preprocessing: A preprocessing layer run before beginning encoding. Usually controls functions such initial embedding. Optional\n",
    "* decoding_postprocessing: A post processing layer run after decoding. Usually used to produce logits or similar contents. Optional.\n",
    "\n",
    "### Method: encode\n",
    "\n",
    "Encodes the latent representation as a sequence of gram embeddings\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* input: A grid of integers. (batch x N x M)\n",
    "* mask: A mask indicating what items are active. (batch x M x N)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* embeddings: The output embeddings after processing (batch x N x M x E)\n",
    "* gram_embeddings: The stack of gram embeddings (batch x L x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We embed, encode positional encodings, and then run the encoding methods in order\n",
    "\n",
    "* Embed: Run the input through the embedding layer. Mask it.\n",
    "* Build encodings: Build the positional encodings\n",
    "* Run layers: For each BGCCEll, moving forward\n",
    "    * Call .encode\n",
    "    * Store resulting gram embedding\n",
    "    * update current embedding\n",
    "    * if relevant, store intermediate embedding\n",
    "* Stack gram_embeddings, if relevant stack intermediate embeddings\n",
    "* return contents\n",
    "\n",
    "### Method: decode\n",
    "\n",
    "Decodes a latent representation based on gram embeddings back into the original content. Does this by running the layers in reverse\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: A grid of pixel embeddings. (batch x N x M x E). May be filled with zeros\n",
    "* mask: A activity mask. (batch x N x M). True means include\n",
    "* gram_embeddings: The gram embeddings. (batch x L x E)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* embeddings: The decoded embeddings\n",
    "* logits: A logit projection for each encoding.\n",
    "\n",
    "**Design**\n",
    "\n",
    "We basically run the encode process but in reverse. \n",
    "\n",
    "Start from the embedding and the gram_embeddings. For each gram_embedding, embedding running in reverse, call the associated decode method. Then project the embedding into logits and return."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T00:10:00.274172Z",
     "start_time": "2024-08-06T00:10:00.242901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BidirectionalGramConverter(nn.Module):\n",
    "    \"\"\"\n",
    "    The bidirectional gram converter mechanism. \n",
    "    \n",
    "    Converts an input int grid into an output gram encoding, or a gram encoding\n",
    "    back into an input grid. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 pos_encoding: PositionalEncodings,\n",
    "                 cells: List[BGCCell],\n",
    "                 encoding_preprocessing: nn.Module,\n",
    "                 decoding_postprocessing: nn.Module\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Sets up the BGC layer based on the provided parameters.\n",
    "        :param pos_encoding: The positional encoding object. \n",
    "        :param cells: The BGCCells in encode order. \n",
    "        :param encoding_preprocessing: The encoding preprocessing, such as embedding layers\n",
    "        :param decoding_postprocessing: The decoding postprocessing, like logit layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.pos_encoding = pos_encoding\n",
    "        self.cells = nn.ModuleList(cells)\n",
    "        self.encoding_preprocessing = encoding_preprocessing\n",
    "        self.decoding_postprocessing = decoding_postprocessing\n",
    "        \n",
    "    def encode(self, content: torch.Tensor, mask: torch.Tensor)->Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Performs the BGC encode process\n",
    "        \n",
    "        :param content: The content to encode\n",
    "        :param mask: The mask to apply to encoding\n",
    "        :return: The embeddings, and the gram encodings\n",
    "        \"\"\"\n",
    "        # Perform preprocessing and get the positioanl encodings setup\n",
    "        embeddings = self.encoding_preprocessing(content)\n",
    "        pos_encoding = self.pos_encoding(mask)\n",
    "        \n",
    "        # Encode the gram encodings\n",
    "        gram_encodings = []\n",
    "        for cell in self.cells:\n",
    "            embeddings, gram_encoding = cell.encode(embeddings, pos_encoding, mask)\n",
    "            gram_encodings.append(gram_encoding)\n",
    "        \n",
    "        # stack and return\n",
    "        gram_encodings = torch.stack(gram_encodings, dim=1)\n",
    "        return embeddings, gram_encodings\n",
    "    \n",
    "    def decode(self, mask: torch.Tensor, gram_encodings: torch.Tensor, embeddings: torch.Tensor | None = None)->torch.Tensor:\n",
    "        \n",
    "        # Define positional encoding\n",
    "        \n",
    "        pos_encoding = self.pos_encoding(mask)\n",
    "        \n",
    "        # Handle blank embeddings\n",
    "        \n",
    "        if embeddings is None:\n",
    "            embeddings = torch.zeros_like(pos_encoding)\n",
    "            \n",
    "        # Decode the gram encodings\n",
    "        for cell, gram_encoding in zip(reversed(self.cells), reversed(gram_encodings.unbind(1))):\n",
    "            embeddings = cell.decode(embeddings, pos_encoding, gram_encoding, mask)\n",
    "            \n",
    "        # Perform postprocessing\n",
    "        return self.decoding_postprocessing(embeddings)\n",
    "            \n",
    "    "
   ],
   "id": "1d2ca042bd8b24cf",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e61f6d34ff2a0e37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

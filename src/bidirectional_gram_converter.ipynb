{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bidirectional Gram Converter\n",
    "\n",
    "We will use the Bidirection Gram Grid Converter architecture to convert between gram embeddings and the original representation. This uses autoencoding for initial training, and makes decoding the opposite of encoding. It also encourages the model to encode into the gram encodings\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* If each encoder cell uses a gram embedding to create an update then subtracts it from the input, each decoder cell can reverse it using that same layer to remake the update and then add it.\n",
    "* We can encourage the model to shunt it's information through gram embeddings instead of the main embeddings with the proper loss on the encoder output.\n",
    "* By sprinkling in positional embeddings regularly before convolutions, we can let the model retain the ability to access to positional information.\n",
    "\n",
    "**Design**\n",
    "\n",
    " At a overall level, the encoder/decoder model acts a lot like a ResNet. It consists of cells with residual bypasses that operate more or less on pixel embeddings - however each cell produces both the next pixel embedding in the chain, AND a Gram Encoding of the layer. \n",
    " \n",
    "However, the cells themselves are quite sophisticated, and are designed to allow bidirectional encoding-decoding with almost exactly the same parameters. \n",
    "\n",
    "A BResNet cell has three main components. These are.\n",
    "\n",
    "* Latent Summary stage: Uses internal parameters and actions to create a Gram Encoding. \n",
    "* Latent Decode stage: Uses a provided Gram Encoding and a known grid shape to create an update of the same shape as the summary input.\n",
    "* Merge stage: Either add or subtract the update. Add when encoding. Subtract when decoding. Replaces the residual bypass: Here is your residual now.\n",
    "\n",
    "The effect of this is that the distinction between encoding, and decoding, from the model's perspective is only a distintion in whether you subtract and use the summary stage, or add and use external Gram Encodings. Lets consider one particular cell operating in encoding and decoding mode to see this illustrated\n",
    "\n",
    "**Training**\n",
    "\n",
    "The mechnanism can be trained as an autoencoder. However, we shall motivate the autoencoder to not rely on the encodings output by the encoder by penalizing those encodings when nonzero.\n"
   ],
   "id": "786360fd22dfba7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters and Imports\n",
    "\n",
    "Hyperparameter and imports go here"
   ],
   "id": "37573f92d41cc617"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import unittest\n",
    "from torch import nn\n",
    "from typing import Tuple"
   ],
   "id": "50c30327298a3381"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Primitive Layers\n",
    "\n",
    "We begin to define the various pieces needed here"
   ],
   "id": "2172698eafe84fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pixel Cell\n",
    "\n",
    "The convolution processing cell is really the only architecture-specific piece here. It is specialized for processing embedded pixel data.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need a convolutional processing mechanism for pixel images.\n",
    "* We will need a cell for that.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `embedding_dim`: The dimensions of the embedding.\n",
    "* `num_layers`: The number of layers deep the cell is.\n",
    "* `kernel_size`: The size of the convolutional kernel.\n",
    "* `dropout_prob`: The probability of an element to be zeroed during dropout (default: 0.5).\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: Image embeddings. Shape (batch x N x M x embedding_dim)\n",
    "* `mask` (optional): A mask to apply at the end. Shape (batch x N x M)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: New image embeddings. Shape (batch x N x M x embedding_dim)\n"
   ],
   "id": "7e6e056b65ea83a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PixelCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional cell network. Processes image embeddings without reduction.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 num_layers: int,\n",
    "                 kernel_size: int,\n",
    "                 dropout_prob: float = 0.5):\n",
    "        \"\"\"\n",
    "        Initialize the conv cell\n",
    "        :param embedding_dim: The dimension of the embeddings\n",
    "        :param num_layers: The number of layers to make\n",
    "        :param kernel_size: The kernel size of the convolutional layers\n",
    "        :param dropout_prob: The probability of an element to be zeroed (default: 0.5)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Construct layers\n",
    "        padding_size = (kernel_size - 1) // 2\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Conv2d(embedding_dim, embedding_dim, kernel_size, padding=padding_size),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "            layers.append(layer)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to process embeddings through convolutional layers.\n",
    "        :param embeddings: The input embeddings. Shape (batch x N x M x embedding_dim)\n",
    "        :param mask: Optional mask to apply at the end. Shape (batch x N x M)\n",
    "        :return: The output embeddings. Shape (batch x N x M x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == 4, \"Embeddings must have 4 dimensions (batch x N x M x embedding_dim)\"\n",
    "        assert embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {embeddings.shape[-1]}\"\n",
    "\n",
    "        # Permute to match the expected input shape for Conv2d\n",
    "        channels = embeddings.permute(0, 3, 1, 2)  # shape: (batch x embedding_dim x N x M)\n",
    "        channels = self.layers(channels)  # shape: (batch x embedding_dim x N x M)\n",
    "        channels = self.dropout(channels)  # Apply dropout here\n",
    "        embeddings = channels.permute(0, 2, 3, 1)  # shape: (batch x N x M x embedding_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            assert mask.dim() == 3, \"Mask must have 3 dimensions (batch x N x M)\"\n",
    "            mask = mask.unsqueeze(-1)  # shape: (batch x N x M x 1)\n",
    "            embeddings = embeddings * mask  # Apply mask\n",
    "\n",
    "        return embeddings\n"
   ],
   "id": "b52b3a7a643e246e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestPixelCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of PixelCell\n",
    "        self.embedding_dim = 64\n",
    "        self.num_layers = 3\n",
    "        self.kernel_size = 3\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 8, 8\n",
    "        self.pixel_cell = PixelCell(self.embedding_dim, self.num_layers, self.kernel_size, self.dropout_prob)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.height, self.width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.embedding_dim)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.pixel_cell(embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.pixel_cell(embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.pixel_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")\n",
    "\n",
    "    def test_dropout_effect(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass multiple times to check for dropout effect\n",
    "        outputs = [self.pixel_cell(embeddings) for _ in range(5)]\n",
    "        \n",
    "        # Asserting that the outputs are different due to dropout\n",
    "        different_outputs = any(not torch.equal(outputs[i], outputs[i + 1]) for i in range(len(outputs) - 1))\n",
    "        self.assertTrue(different_outputs, \"Dropout does not seem to have an effect\")\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.height, self.width)\n",
    "        \n",
    "        # Running the forward pass with mask\n",
    "        output = self.pixel_cell(embeddings, mask)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.height, self.width, self.embedding_dim))\n",
    "        \n",
    "        # Check that masking was applied correctly (if mask is all ones, output should be unaffected)\n",
    "        self.assertTrue(torch.equal(output, self.pixel_cell(embeddings)))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        # Creating dummy data for embeddings and partial mask\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.height, self.width)\n",
    "        mask[:, :self.height//2, :self.width//2] = 0  # Zero out the top-left quarter of the mask\n",
    "        \n",
    "        # Running the forward pass with partial mask\n",
    "        output = self.pixel_cell(embeddings, mask)\n",
    "        \n",
    "        # Asserting the masked areas in the output are zeroed\n",
    "        self.assertTrue(torch.equal(output[:, :self.height//2, :self.width//2, :], torch.zeros_like(output[:, :self.height//2, :self.width//2, :])))\n"
   ],
   "id": "759dbfed6fa72c08"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TextCell\n",
    "\n",
    "Designed for processing a stream of text embeddings. I am not sure if I will use it\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* num_layers: The number of encoding cells to use.\n",
    "* embedding_dim: The width of each individual embedding.\n",
    "* num_heads: The number of transformer heads.\n",
    "* dim_feedforward: The size of the feedforward layer\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: A tensor of text embeddings. Shape (batch x N x Embeddings)\n",
    "* mask: A mask of active text embeddings. Shape (batch x N x Embedding)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* Embeddings: An output sequence of embeddings. Shape (batch x N x Embeddings)\n",
    "\n",
    "**Design**\n",
    "\n",
    "Basically, we use a sequence of transformer encoder layers to encode the embeddings."
   ],
   "id": "ef3a3b6a4a2fc1fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TextCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based text processing cell. Processes a stream of text embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_layers: int,\n",
    "                 embedding_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dim_feedforward: int,\n",
    "                 dropout_prob: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the transformer-based text cell\n",
    "        :param num_layers: The number of encoding cells to use.\n",
    "        :param embedding_dim: The width of each individual embedding.\n",
    "        :param num_heads: The number of transformer heads.\n",
    "        :param dim_feedforward: The size of the feedforward layer.\n",
    "        :param dropout_prob: The probability of an element to be zeroed (default: 0.1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Construct transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout_prob,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to process text embeddings through transformer encoder layers.\n",
    "        :param embeddings: A tensor of text embeddings. Shape (batch x N x embedding_dim)\n",
    "        :param mask: A mask of active text embeddings. Shape (batch x N)\n",
    "        :return: An output sequence of embeddings. Shape (batch x N x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == 3, \"Embeddings must have 3 dimensions (batch x N x embedding_dim)\"\n",
    "        assert embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {embeddings.shape[-1]}\"\n",
    "\n",
    "        # Pass through transformer encoder with optional mask\n",
    "        if mask is not None:\n",
    "            assert mask.dim() == 2, \"Mask must have 2 dimensions (batch x N)\"\n",
    "        output = self.transformer_encoder(embeddings, src_key_padding_mask=mask)\n",
    "\n",
    "        return output"
   ],
   "id": "7738923381a9d77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestTextCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of TextCell\n",
    "        self.num_layers = 4\n",
    "        self.embedding_dim = 64\n",
    "        self.num_heads = 8\n",
    "        self.dim_feedforward = 256\n",
    "        self.dropout_prob = 0.1\n",
    "        self.batch_size = 2\n",
    "        self.seq_length = 10\n",
    "        self.text_cell = TextCell(self.num_layers, self.embedding_dim, self.num_heads, self.dim_feedforward, self.dropout_prob)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim, 2)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.text_cell(embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.text_cell(embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for embeddings\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.text_cell(embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.seq_length, dtype=torch.bool)\n",
    "        \n",
    "        # Running the forward pass with mask\n",
    "        output = self.text_cell(embeddings, mask)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n",
    "        \n",
    "        # Check that masking was applied correctly (if mask is all ones, output should be unaffected)\n",
    "        self.assertTrue(torch.equal(output, self.text_cell(embeddings)))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        # Creating dummy data for embeddings and partial mask\n",
    "        embeddings = torch.randn(self.batch_size, self.seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.seq_length, dtype=torch.bool)\n",
    "        mask[:, :self.seq_length//2] = False  # Zero out the first half of the sequence for the mask\n",
    "        \n",
    "        # Running the forward pass with partial mask\n",
    "        output = self.text_cell(embeddings, mask)\n",
    "        \n",
    "        # Since the mask is True (or 1) where tokens are valid and False (or 0) where they are not,\n",
    "        # The masked parts of the output should be unaffected.\n",
    "        # No straightforward way to check without knowing transformer internals or ground truth values.\n",
    "        # For simplicity, we'll just check the output shape here.\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embedding_dim))\n"
   ],
   "id": "9197dfb5b93a771f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GramEncoder\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need something to convert a sequence of embeddings into a reduced-dimensions gram embedding.\n",
    "* It is easier to process gram matrices back to embeddings when the matrix dimensions are small\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* embedding_dim: The embeddin dimensions.\n",
    "* num_heads: The number of encoding heads\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* Embeddings: The embeddings to process. (batch x ... x E)\n",
    "* Mask: The masked embeddings. (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* Gram Embedding: The gram embedding for the situation. (batch x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "Basically, we make heads as in a transformer, then gram encode the heads, then recombine the results and feedforward. This will involve significantly less needed parameters than directly processing the encoding. In specific, we:\n",
    "\n",
    "* Mask the embeddings.\n",
    "* flatten into 1d embeddings.\n",
    "* reshape the input embedding dim into num_heads.\n",
    "* create and flatten gram matrices for each head. \n",
    "* concatenate the heads together and run them through a linear projection back into embedding_dim\n",
    "\n"
   ],
   "id": "2338dd26ae28bb64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T20:31:08.135368Z",
     "start_time": "2024-07-30T20:31:08.104115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GramEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes embeddings as gram encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 num_heads: int):\n",
    "        \"\"\"\n",
    "        :param embedding_dim: The dimension of the embeddings\n",
    "        :param num_heads: The number of heads\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_dim = embedding_dim // num_heads\n",
    "        self.encode_dim = self.head_dim ** 2\n",
    "\n",
    "        self.combine = nn.Linear(self.encode_dim * self.num_heads, self.embedding_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_gram_encodings(embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create gram encodings from embeddings. These are created from a mean.\n",
    "        :param embeddings: The embeddings. (batch x num_heads x L x head_dim)\n",
    "        :param mask: The mask. (batch x L)\n",
    "        :return: The encodings. (batch x num_heads x head_dim^2)\n",
    "        \"\"\"\n",
    "        # Apply mask\n",
    "        embeddings = embeddings * mask.unsqueeze(1).unsqueeze(-1)\n",
    "\n",
    "        # Compute the gram matrix\n",
    "        gram_matrix = torch.einsum('bnhl,bnhk->bnhk', embeddings, embeddings)  # Shape (batch x num_heads x head_dim x head_dim)\n",
    "        encodings = gram_matrix.flatten(2, -1)  # shape (batch x num_heads x head_dim^2)\n",
    "\n",
    "        # Normalize the gram encodings by the active entries that contributed. This takes the mean and handles variable sizes.\n",
    "        counts = mask.sum(dim=-1, keepdim=True)  # (batch x 1)\n",
    "        counts = counts.unsqueeze(1)  # shape: (batch x 1 x 1)\n",
    "        encodings = encodings / (counts + 1e-8)  # shape (batch x num_heads x head_dim^2)\n",
    "\n",
    "        return encodings\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward method to produce gram embeddings\n",
    "        :param embeddings: The input embeddings. Shape (batch_size x ... x E)\n",
    "        :param mask: The input mask. Shape (batch_size x ...)\n",
    "        :return: The gram embedding. Shape (batch_size x E)\n",
    "        \"\"\"\n",
    "        # Check input shapes\n",
    "        assert embeddings.dim() >= 3, \"Embeddings must have at least 3 dimensions (batch_size, ..., E)\"\n",
    "        assert mask.dim() == embeddings.dim() - 1, \"Mask must have one less dimension than embeddings\"\n",
    "\n",
    "        # Flatten\n",
    "        batch_size = embeddings.size(0)\n",
    "        embeddings = embeddings.flatten(1, -2)  # Shape(batch_size, L, E)\n",
    "        mask = mask.flatten(1, -1)  # Shape (batch x L)\n",
    "\n",
    "        # Create heads\n",
    "        heads = embeddings.view(batch_size, -1, self.num_heads, self.head_dim)  # Shape (batch x L x num_heads x head_dim)\n",
    "        heads = heads.permute(0, 2, 1, 3)  # Shape (batch x num_heads x L x head_dim)\n",
    "\n",
    "        # Process gram encodings\n",
    "        encodings = self.create_gram_encodings(heads, mask)  # (batch x num_heads x head_dim^2)\n",
    "\n",
    "        # Recombine\n",
    "        encodings = encodings.flatten(1, -1)  # shape: (batch x num_heads * head_dim^2)\n",
    "        embeddings = self.combine(encodings)  # shape: (batch x E)\n",
    "        return embeddings"
   ],
   "id": "4873556974e9a710",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 5 (8195848.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[1], line 9\u001B[1;36m\u001B[0m\n\u001B[1;33m    \u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block after function definition on line 5\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestGramEncoder(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of GramEncoder\n",
    "        self.embedding_dim = 64\n",
    "        self.num_heads = 4\n",
    "        self.batch_size = 2\n",
    "        self.height, self.width = 8, 8\n",
    "        self.encoder = GramEncoder(self.embedding_dim, self.num_heads)\n",
    "\n",
    "    def test_gram_encoding(self):\n",
    "        # Creating dummy data for embeddings and mask\n",
    "        embeddings = torch.randn(self.batch_size, self.height, self.width, self.embedding_dim)\n",
    "        mask = torch.ones(self.batch_size, self.height, self.width)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        gram_embedding = self.encoder(embeddings, mask)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(gram_embedding.shape, (self.batch_size, self.embedding_dim))\n",
    "        \n",
    "        # Additional tests can be added here to check specific values or properties\n",
    "        # Example: check if the output is not NaN\n",
    "        self.assertFalse(torch.isnan(gram_embedding).any(), \"Output contains NaNs\")\n",
    "        \n",
    "        # Example: check if the output is not infinite\n",
    "        self.assertFalse(torch.isinf(gram_embedding).any(), \"Output contains infinite values\")\n"
   ],
   "id": "fba11f68377bca1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feedforward\n",
    "\n",
    "The feedforward layer works a lot like a transformers. It lets the model decide to respond differently with different encoding inputs\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* embedding_dim: The dimension of the embedding input\n",
    "* feedforward_dim: The dimension of the internal feedforward channel\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "*gram_embeddings: Embeddings. Shape (batch x embeddings)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "*gram_embeddings: Embeddings. Shape (batch x embeddings)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We have a linear, a relu, and a linear. Not much else to say."
   ],
   "id": "c58e64b4b3b6c2df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward layer. Works similarly to a transformer feedforward layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, feedforward_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the feedforward layer\n",
    "        :param embedding_dim: The dimension of the embedding input.\n",
    "        :param feedforward_dim: The dimension of the internal feedforward channel.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "\n",
    "        # Define the feedforward network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, feedforward_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feedforward_dim, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, gram_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the feedforward network.\n",
    "        :param gram_embeddings: Embeddings. Shape (batch x embedding_dim)\n",
    "        :return: Embeddings. Shape (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert gram_embeddings.dim() == 2, \"gram_embeddings must have 2 dimensions (batch x embedding_dim)\"\n",
    "        assert gram_embeddings.shape[-1] == self.embedding_dim, f\"Expected embedding dimension {self.embedding_dim}, but got {gram_embeddings.shape[-1]}\"\n",
    "\n",
    "        return self.network(gram_embeddings)"
   ],
   "id": "668a914724a22c58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestFeedforward(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of Feedforward\n",
    "        self.embedding_dim = 64\n",
    "        self.feedforward_dim = 256\n",
    "        self.batch_size = 32\n",
    "        self.feedforward_layer = Feedforward(self.embedding_dim, self.feedforward_dim)\n",
    "\n",
    "    def test_forward_shape(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        # Creating dummy data with invalid number of dimensions\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim, 2)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid input dimensions\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.feedforward_layer(gram_embeddings)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        # Creating dummy data with invalid embedding dimension\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim + 1)\n",
    "        \n",
    "        # Asserting that an AssertionError is raised for invalid embedding dimension\n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.feedforward_layer(gram_embeddings)\n",
    "\n",
    "    def test_no_nan_values(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain NaNs\n",
    "        self.assertFalse(torch.isnan(output).any(), \"Output contains NaNs\")\n",
    "\n",
    "    def test_no_inf_values(self):\n",
    "        # Creating dummy data for gram_embeddings\n",
    "        gram_embeddings = torch.randn(self.batch_size, self.embedding_dim)\n",
    "        \n",
    "        # Running the forward pass\n",
    "        output = self.feedforward_layer(gram_embeddings)\n",
    "        \n",
    "        # Asserting the output does not contain infinite values\n",
    "        self.assertFalse(torch.isinf(output).any(), \"Output contains infinite values\")"
   ],
   "id": "c6abb3a3a38283e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Architecture Layers\n",
    "\n",
    "The main architecture layers and their mechanisms. Everything here forward is setup using dependency injection."
   ],
   "id": "befe1e32086315a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BGCEncoderCell\n",
    "\n",
    "An encoder cell that takes in an image embedding and produces a gram embedding. \n",
    "\n",
    "**Premise**\n",
    "\n",
    "We need to be able to encode an input into a gram embedding for this architecture to work.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* encoder: The encoding stack.\n",
    "* gram_encoder: The gram encoder\n",
    "* feedforward: A feedforward network that goes off against the gram embeddings\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: a batch of 2d positional embeddings: (batch x ... x E)\n",
    "* mask: a mask for the batch elements, True means active: (batch x ...)\n",
    "* pos_encoding: a grid of positional encodings to inject. Shape (batch x ... x E)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* gram_embedding: A gram embedding. Shape (batch x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "The layer starts by injecting the positional encodings into the input, then processes them using\n",
    "the encoder. The result is passed into the GramEncoder, which is then passed back out.\n"
   ],
   "id": "d5681aa1559230c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BGCEncoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder cell that takes in a collection of embeddings and produces a gram embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder: nn.Module, gram_encoder: nn.Module, feedforward: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCEncoderCell\n",
    "        :param encoder: The encoding stack.\n",
    "        :param gram_encoder: The gram encoder.\n",
    "        :param feedforward: A feedforward network that goes off against the gram embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.gram_encoder = gram_encoder\n",
    "        self.feedforward = feedforward\n",
    "\n",
    "    def forward(self, embeddings: torch.Tensor, mask: torch.Tensor, pos_encoding: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to produce a gram embedding from input embeddings.\n",
    "        :param embeddings: A batch of positional embeddings. Shape (batch x ... x embedding_dim)\n",
    "        :param mask: A mask for the batch elements, True means active. Shape (batch x ...)\n",
    "        :param pos_encoding: A grid of positional encodings to inject. Shape (batch x ... x embedding_dim)\n",
    "        :return: A gram embedding. Shape (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        assert embeddings.dim() == pos_encoding.dim(), \"Embeddings and positional encodings must have the same number of dimensions\"\n",
    "        assert embeddings.shape[-1] == pos_encoding.shape[-1], \"Embeddings and positional encodings must have the same embedding dimension\"\n",
    "        assert embeddings.shape[:-1] == mask.shape, \"Embeddings and mask must have the same batch and spatial dimensions\"\n",
    "\n",
    "        # Inject positional encodings\n",
    "        embeddings = embeddings + pos_encoding\n",
    "\n",
    "        # Process with the encoder\n",
    "        encoded = self.encoder(embeddings, mask)\n",
    "\n",
    "        # Produce the gram embedding\n",
    "        gram_embedding = self.gram_encoder(encoded, mask)\n",
    "\n",
    "        # Pass the gram embedding through the feedforward network\n",
    "        gram_embedding = self.feedforward(gram_embedding)\n",
    "\n",
    "        return gram_embedding"
   ],
   "id": "8192c797ad76451c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestBGCEncoderCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of BGCEncoderCell\n",
    "        self.embedding_dim = 64\n",
    "\n",
    "        # Dummy modules for encoder, gram_encoder, and feedforward\n",
    "        class DummyEncoder(nn.Module):\n",
    "            def forward(self, x, mask):\n",
    "                return x\n",
    "\n",
    "        class DummyGramEncoder(nn.Module):\n",
    "            def forward(self, x, mask):\n",
    "                return torch.mean(x, dim=tuple(range(1, x.dim() - 1)))\n",
    "\n",
    "        class DummyFeedforward(nn.Module):\n",
    "            def forward(self, x):\n",
    "                return x\n",
    "\n",
    "        self.encoder = DummyEncoder()\n",
    "        self.gram_encoder = DummyGramEncoder()\n",
    "        self.feedforward = DummyFeedforward()\n",
    "\n",
    "        self.bgc_encoder_cell = BGCEncoderCell(self.encoder, self.gram_encoder, self.feedforward)\n",
    "\n",
    "    def test_forward_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_3d(self):\n",
    "        batch_size = 2\n",
    "        depth, height, width = 4, 8, 8\n",
    "        embeddings = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, depth, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim, 2)\n",
    "        mask = torch.ones(batch_size, seq_length, 2)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim, 2)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim + 1)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim + 1)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "        \n",
    "        # Check that masking was applied correctly (if mask is all ones, output should be unaffected)\n",
    "        self.assertTrue(torch.equal(output, self.bgc_encoder_cell(embeddings, mask, pos_encoding)))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        mask[:, :seq_length // 2] = 0  # Zero out the first half of the sequence for the mask\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        \n",
    "        output = self.bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim))\n",
    "        \n",
    "        # Since the mask is True (or 1) where tokens are valid and False (or 0) where they are not,\n",
    "        # The masked parts of the output should be unaffected.\n",
    "        # No straightforward way to check without knowing encoder internals or ground truth values.\n",
    "        # For simplicity, we'll just check the output shape here."
   ],
   "id": "fd2d778d536545d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_bgc_encoder_cell_for_text(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_feedforward: int,\n",
    "        dropout_prob: float,\n",
    "        gram_feedforward: int,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCEncoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCEncoderCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param transformer_heads: The number of transformer heads in the TextCell.\n",
    "    :param transformer_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :param gram_feedforward: The dimension of the internal feedforward network for the gram encoder.\n",
    "    :param gram_heads: The number of heads in the GramEncoder.\n",
    "    :return: An instance of BGCEncoderCell configured for text data.\n",
    "    \"\"\"\n",
    "    encoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=transformer_heads,\n",
    "        dim_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=gram_feedforward\n",
    "    )\n",
    "    return BGCEncoderCell(encoder, gram_encoder, feedforward)\n",
    "\n",
    "def build_bgc_encoder_cell_for_pixel(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        kernel_size: int,\n",
    "        dropout_prob: float,\n",
    "        gram_feedforward: int,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCEncoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCEncoderCell for pixel data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :param gram_feedforward: The dimension of the internal feedforward network for the gram encoder.\n",
    "    :param gram_heads: The number of heads in the GramEncoder.\n",
    "    :return: An instance of BGCEncoderCell configured for pixel data.\n",
    "    \"\"\"\n",
    "    encoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=gram_feedforward\n",
    "    )\n",
    "    return BGCEncoderCell(encoder, gram_encoder, feedforward)"
   ],
   "id": "7f048060e1e8dd66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestBGCEncoderCellBuilders(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters for text and pixel BGCEncoderCell builders\n",
    "        self.num_layers_text = 4\n",
    "        self.embedding_dim_text = 64\n",
    "        self.num_heads_text = 8\n",
    "        self.dim_feedforward_text = 256\n",
    "        self.dropout_prob_text = 0.1\n",
    "        self.feedforward_dim_text = 256\n",
    "        self.gram_heads_text = 8\n",
    "\n",
    "        self.num_layers_pixel = 3\n",
    "        self.embedding_dim_pixel = 64\n",
    "        self.kernel_size_pixel = 3\n",
    "        self.dropout_prob_pixel = 0.5\n",
    "        self.feedforward_dim_pixel = 256\n",
    "        self.gram_heads_pixel = 8\n",
    "\n",
    "    def test_build_bgc_encoder_cell_for_text(self):\n",
    "        text_bgc_encoder_cell = build_bgc_encoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            num_heads=self.num_heads_text,\n",
    "            dim_feedforward=self.dim_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text,\n",
    "            feedforward_dim=self.feedforward_dim_text,\n",
    "            gram_heads=self.gram_heads_text\n",
    "        )\n",
    "        self.assertIsInstance(text_bgc_encoder_cell, BGCEncoderCell)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.encoder, TextCell)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.gram_encoder, GramEncoder)\n",
    "        self.assertIsInstance(text_bgc_encoder_cell.feedforward, Feedforward)\n",
    "\n",
    "    def test_build_bgc_encoder_cell_for_pixel(self):\n",
    "        pixel_bgc_encoder_cell = build_bgc_encoder_cell_for_pixel(\n",
    "            num_layers=self.num_layers_pixel,\n",
    "            embedding_dim=self.embedding_dim_pixel,\n",
    "            kernel_size=self.kernel_size_pixel,\n",
    "            dropout_prob=self.dropout_prob_pixel,\n",
    "            feedforward_dim=self.feedforward_dim_pixel,\n",
    "            gram_heads=self.gram_heads_pixel\n",
    "        )\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell, BGCEncoderCell)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.encoder, PixelCell)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.gram_encoder, GramEncoder)\n",
    "        self.assertIsInstance(pixel_bgc_encoder_cell.feedforward, Feedforward)\n",
    "\n",
    "    def test_integration_text_bgc_encoder_cell(self):\n",
    "        text_bgc_encoder_cell = build_bgc_encoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            num_heads=self.num_heads_text,\n",
    "            dim_feedforward=self.dim_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text,\n",
    "            feedforward_dim=self.feedforward_dim_text,\n",
    "            gram_heads=self.gram_heads_text\n",
    "        )\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "        \n",
    "        output = text_bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim_text))\n",
    "\n",
    "    def test_integration_pixel_bgc_encoder_cell(self):\n",
    "        pixel_bgc_encoder_cell = build_bgc_encoder_cell_for_pixel(\n",
    "            num_layers=self.num_layers_pixel,\n",
    "            embedding_dim=self.embedding_dim_pixel,\n",
    "            kernel_size=self.kernel_size_pixel,\n",
    "            dropout_prob=self.dropout_prob_pixel,\n",
    "            feedforward_dim=self.feedforward_dim_pixel,\n",
    "            gram_heads=self.gram_heads_pixel\n",
    "        )\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim_pixel)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim_pixel)\n",
    "        \n",
    "        output = pixel_bgc_encoder_cell(embeddings, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, self.embedding_dim_pixel))"
   ],
   "id": "ef4bce6861d35351"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BGCDecoderCell\n",
    "\n",
    "A decode cell turns a gram embedding into an update that can be applied.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "We need to be able to create updates from gram embeddings in order to encode and decode.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `decoder`: The primary decoder mechanism, likely a stack of convolutional networks or similar.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `gram_embedding`: The gram embedding of the layer. Shape: (batch x embedding_dim).\n",
    "* `mask`: The mask indicating active elements. Shape: (batch x ...).\n",
    "* `pos_encodings`: The positional encodings to use. Shape: (batch x ... x embedding_dim).\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `update`: The update built from the gram embeddings. Shape: (batch x ... x embedding_dim).\n",
    "\n",
    "**Design**\n",
    "\n",
    "We follow the following steps:\n",
    "\n",
    "* Expand the `gram_embedding` to match the shape of `pos_encodings`.\n",
    "* Add the positional encodings to the expanded gram embedding and multiply by the mask.\n",
    "* Run the combined input through the decoder layer.\n",
    "* Return the result as the update.\n"
   ],
   "id": "4e5b4cbe747bebb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BGCDecoderCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A decode cell turns a gram embedding into an update that can be applied.\n",
    "    \"\"\"\n",
    "    def __init__(self, decoder: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCDecoderCell\n",
    "        :param decoder: The primary decoder mechanism. Likely a stack of convolutional networks.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, gram_embedding: torch.Tensor, mask: torch.Tensor, pos_encodings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to produce an update from gram embeddings.\n",
    "        :param gram_embedding: The gram embedding of the layer. Shape (batch x embedding_dim).\n",
    "        :param mask: The mask indicating active elements. Shape (batch x ...).\n",
    "        :param pos_encodings: The positional encodings to use. Shape (batch x ... x embedding_dim).\n",
    "        :return: The update built from the gram embeddings. Shape: (batch x ... x embedding_dim).\n",
    "        \"\"\"\n",
    "        assert gram_embedding.dim() == 2, \"Gram embedding must have 2 dimensions (batch x embedding_dim)\"\n",
    "        assert pos_encodings.shape[:-1] == mask.shape, \"Positional encodings and mask must have the same shape except for the last dimension\"\n",
    "        assert pos_encodings.shape[-1] == gram_embedding.shape[-1], \"Positional encodings and gram embeddings must have the same embedding dimension\"\n",
    "\n",
    "        # Expand gram_embedding to match pos_encoding shape.\n",
    "        while gram_embedding.dim() < pos_encodings.dim():\n",
    "            gram_embedding = gram_embedding.unsqueeze(1)\n",
    "\n",
    "        # Take the positional encodings, add it to the expanded gram embedding, and multiply by the mask\n",
    "        combined_input = (gram_embedding + pos_encodings) * mask.unsqueeze(-1)\n",
    "\n",
    "        # Run the combined input through the decoder\n",
    "        update = self.decoder(combined_input)\n",
    "\n",
    "        return update"
   ],
   "id": "3937f2f5edeca302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class DummyDecoder(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class TestBGCDecoderCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters and creating an instance of BGCDecoderCell\n",
    "        self.decoder = DummyDecoder()\n",
    "        self.bgc_decoder_cell = BGCDecoderCell(self.decoder)\n",
    "        self.embedding_dim = 64\n",
    "\n",
    "    def test_forward_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_shape_3d(self):\n",
    "        batch_size = 2\n",
    "        depth, height, width = 4, 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, depth, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, depth, height, width)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, depth, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_forward_invalid_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim, 2)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_invalid_embedding_dim(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim + 1)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        with self.assertRaises(AssertionError):\n",
    "            self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "\n",
    "    def test_forward_with_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_forward_with_partial_mask(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        mask[:, :seq_length // 2] = 0  # Zero out the first half of the sequence for the mask\n",
    "        \n",
    "        output = self.bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim))"
   ],
   "id": "d4cf6278bec25ad9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_bgc_decoder_cell_for_text(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_heads: int,\n",
    "        transformer_feedforward: int,\n",
    "        dropout_prob: float,\n",
    "        **kwargs\n",
    "    ) -> BGCDecoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCDecoderCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param transformer_heads: The number of transformer heads in the TextCell.\n",
    "    :param transformer_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :return: An instance of BGCDecoderCell configured for text data.\n",
    "    \"\"\"\n",
    "    decoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=transformer_heads,\n",
    "        dim_feedforward=transformer_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    return BGCDecoderCell(decoder)\n",
    "\n",
    "def build_bgc_decoder_cell_for_image(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        kernel_size: int,\n",
    "        dropout_prob: float,\n",
    "        **kwargs\n",
    "    ) -> BGCDecoderCell:\n",
    "    \"\"\"\n",
    "    Build a BGCDecoderCell for image data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :return: An instance of BGCDecoderCell configured for image data.\n",
    "    \"\"\"\n",
    "    decoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    return BGCDecoderCell(decoder)\n"
   ],
   "id": "ed4af20c9147076a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestBGCDecoderCellBuilders(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Setting up the necessary parameters for text and image BGCDecoderCell builders\n",
    "        self.num_layers_text = 4\n",
    "        self.embedding_dim_text = 64\n",
    "        self.transformer_heads_text = 8\n",
    "        self.transformer_feedforward_text = 256\n",
    "        self.dropout_prob_text = 0.1\n",
    "\n",
    "        self.num_layers_image = 3\n",
    "        self.embedding_dim_image = 64\n",
    "        self.kernel_size_image = 3\n",
    "        self.dropout_prob_image = 0.5\n",
    "\n",
    "    def test_build_bgc_decoder_cell_for_text(self):\n",
    "        text_bgc_decoder_cell = build_bgc_decoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text\n",
    "        )\n",
    "        self.assertIsInstance(text_bgc_decoder_cell, BGCDecoderCell)\n",
    "        self.assertIsInstance(text_bgc_decoder_cell.decoder, TextCell)\n",
    "\n",
    "    def test_build_bgc_decoder_cell_for_image(self):\n",
    "        image_bgc_decoder_cell = build_bgc_decoder_cell_for_image(\n",
    "            num_layers=self.num_layers_image,\n",
    "            embedding_dim=self.embedding_dim_image,\n",
    "            kernel_size=self.kernel_size_image,\n",
    "            dropout_prob=self.dropout_prob_image\n",
    "        )\n",
    "        self.assertIsInstance(image_bgc_decoder_cell, BGCDecoderCell)\n",
    "        self.assertIsInstance(image_bgc_decoder_cell.decoder, PixelCell)\n",
    "\n",
    "    def test_integration_text_bgc_decoder_cell(self):\n",
    "        text_bgc_decoder_cell = build_bgc_decoder_cell_for_text(\n",
    "            num_layers=self.num_layers_text,\n",
    "            embedding_dim=self.embedding_dim_text,\n",
    "            transformer_heads=self.transformer_heads_text,\n",
    "            transformer_feedforward=self.transformer_feedforward_text,\n",
    "            dropout_prob=self.dropout_prob_text\n",
    "        )\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim_text)\n",
    "        pos_encoding = torch.randn(batch_size, seq_length, self.embedding_dim_text)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        output = text_bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, seq_length, self.embedding_dim_text))\n",
    "\n",
    "    def test_integration_image_bgc_decoder_cell(self):\n",
    "        image_bgc_decoder_cell = build_bgc_decoder_cell_for_image(\n",
    "            num_layers=self.num_layers_image,\n",
    "            embedding_dim=self.embedding_dim_image,\n",
    "            kernel_size=self.kernel_size_image,\n",
    "            dropout_prob=self.dropout_prob_image\n",
    "        )\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        gram_embedding = torch.randn(batch_size, self.embedding_dim_image)\n",
    "        pos_encoding = torch.randn(batch_size, height, width, self.embedding_dim_image)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        output = image_bgc_decoder_cell(gram_embedding, mask, pos_encoding)\n",
    "        self.assertEqual(output.shape, (batch_size, height, width, self.embedding_dim_image))"
   ],
   "id": "47112207df4e9b6a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BGCCell\n",
    "\n",
    "A decoder/encoder layer capable of performing the decoding or encoding action on demand.\n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We need to specify a cell to actually do the BGC process.\n",
    "* It can elegantly support bidirectionality if we subtract when decoding and add when encoding.\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "* `encoder_cell` [BGCEncoderCell]: Encodes an input into gram embeddings.\n",
    "* `decoder_cell` [BCGDecoderCell]: Starts from a gram embedding and the shape, and produces an update.\n",
    "\n",
    "### Method: Encode\n",
    "\n",
    "This is the encode action for the cell.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: A batch of embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `pos_encodings`: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "* `mask`: A mask indicating active elements. Shape: (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `gram_embeddings`: The gram embedding for the layer. Shape: (batch x embedding_dim)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We follow the following sequence of events:\n",
    "\n",
    "* Use the `encoder_cell` to create a `gram_embedding` from the inputs.\n",
    "* Use the `decoder_cell` to create an 'update' from the `gram_embedding`.\n",
    "* Subtract the 'update' from the original embeddings.\n",
    "* Return the embeddings and `gram_embedding`.\n",
    "\n",
    "### Method: Decode\n",
    "\n",
    "Runs a decode action for the layer. Conceptually, this is like the encode action but in reverse.\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* `embeddings`: The current embeddings. Shape: (batch x ... x embedding_dim)\n",
    "* `pos_encodings`: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "* `gram_encoding`: The gram encoding for the layer. Shape: (batch x embedding_dim)\n",
    "* `mask`: The mask for the layer. Shape: (batch x ...)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "* `embeddings`: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We proceed similarly to encoding, except we use the decoder cell, and we add the update instead:\n",
    "\n",
    "* Use the `decoder_cell` to create an 'update' from the `gram_encoding`.\n",
    "* Add the update to the embeddings.\n",
    "* Return the resulting embeddings.\n"
   ],
   "id": "ccd80a79224cd291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BGCCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder/encoder layer capable of performing the decoding or encoding action on demand.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_cell: nn.Module, decoder_cell: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BGCCell\n",
    "        :param encoder_cell: Encodes an input into gram embeddings.\n",
    "        :param decoder_cell: Starts from a gram embedding and the shape, and produces an update.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder_cell = encoder_cell\n",
    "        self.decoder_cell = decoder_cell\n",
    "\n",
    "    def encode(self, embeddings: torch.Tensor, pos_encodings: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode action for the cell.\n",
    "        :param embeddings: A batch of embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        :param pos_encodings: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "        :param mask: A mask indicating active elements. Shape: (batch x ...)\n",
    "        :return: embeddings: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "                 gram_embeddings: The gram embedding for the layer. Shape: (batch x embedding_dim)\n",
    "        \"\"\"\n",
    "        # Use the encoder_cell to create a gram_embedding from the inputs\n",
    "        gram_embedding = self.encoder_cell(embeddings, mask, pos_encodings)\n",
    "\n",
    "        # Use the decoder_cell to create an 'update' from the gram_embedding\n",
    "        update = self.decoder_cell(gram_embedding, mask, pos_encodings)\n",
    "\n",
    "        # Subtract the 'update' from the original embeddings\n",
    "        embeddings = embeddings - update\n",
    "\n",
    "        return embeddings, gram_embedding\n",
    "\n",
    "    def decode(self, embeddings: torch.Tensor, pos_encodings: torch.Tensor, gram_encoding: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode action for the cell.\n",
    "        :param embeddings: The current embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        :param pos_encodings: Positional encodings. Shape: (batch x ... x embedding_dim)\n",
    "        :param gram_encoding: The gram encoding for the layer. Shape: (batch x embedding_dim)\n",
    "        :param mask: The mask for the layer. Shape: (batch x ...)\n",
    "        :return: The output embeddings. Shape: (batch x ... x embedding_dim)\n",
    "        \"\"\"\n",
    "        # Use the decoder_cell to create an 'update' from the gram_encoding\n",
    "        update = self.decoder_cell(gram_encoding, mask, pos_encodings)\n",
    "\n",
    "        # Add the update to the embeddings\n",
    "        embeddings = embeddings + update\n",
    "\n",
    "        return embeddings"
   ],
   "id": "a6747d035ca0fb0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class DummyEncoderCell(nn.Module):\n",
    "    def forward(self, embeddings, mask, pos_encodings):\n",
    "        # Flatten everything between the first and last dimension\n",
    "        flattened_embeddings = embeddings.flatten(start_dim=1, end_dim=-2)\n",
    "        # Take the mean of the middle dimension\n",
    "        return torch.mean(flattened_embeddings, dim=1)  # Simplified\n",
    "\n",
    "class DummyDecoderCell(nn.Module):\n",
    "    def forward(self, gram_embedding, mask, pos_encodings):\n",
    "        # Just return the pos_encodings as it has the right shape\n",
    "        return pos_encodings\n",
    "\n",
    "class TestBGCCell(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embedding_dim = 64\n",
    "        self.encoder_cell = DummyEncoderCell()\n",
    "        self.decoder_cell = DummyDecoderCell()\n",
    "        self.bgc_cell = BGCCell(self.encoder_cell, self.decoder_cell)\n",
    "\n",
    "    def test_encode_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        self.assertEqual(encoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "        self.assertEqual(gram_embedding.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_encode_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        self.assertEqual(encoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n",
    "        self.assertEqual(gram_embedding.shape, (batch_size, self.embedding_dim))\n",
    "\n",
    "    def test_decode_shape_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        gram_encoding = torch.randn(batch_size, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "        \n",
    "        decoded_embeddings = self.bgc_cell.decode(embeddings, pos_encodings, gram_encoding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_decode_shape_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        gram_encoding = torch.randn(batch_size, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "        \n",
    "        decoded_embeddings = self.bgc_cell.decode(embeddings, pos_encodings, gram_encoding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n",
    "\n",
    "    def test_encode_decode_consistency_1d(self):\n",
    "        batch_size = 2\n",
    "        seq_length = 10\n",
    "        embeddings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, seq_length, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, seq_length)\n",
    "\n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        decoded_embeddings = self.bgc_cell.decode(encoded_embeddings, pos_encodings, gram_embedding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, seq_length, self.embedding_dim))\n",
    "\n",
    "    def test_encode_decode_consistency_2d(self):\n",
    "        batch_size = 2\n",
    "        height, width = 8, 8\n",
    "        embeddings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        pos_encodings = torch.randn(batch_size, height, width, self.embedding_dim)\n",
    "        mask = torch.ones(batch_size, height, width)\n",
    "\n",
    "        encoded_embeddings, gram_embedding = self.bgc_cell.encode(embeddings, pos_encodings, mask)\n",
    "        decoded_embeddings = self.bgc_cell.decode(encoded_embeddings, pos_encodings, gram_embedding, mask)\n",
    "        self.assertEqual(decoded_embeddings.shape, (batch_size, height, width, self.embedding_dim))\n"
   ],
   "id": "82d01e8859841475"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_bgc_cell_for_text(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout_prob: float,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCCell:\n",
    "    \"\"\"\n",
    "    Build a BGCCell for text data.\n",
    "    :param num_layers: Number of layers in the TextCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param num_heads: The number of transformer heads in the TextCell.\n",
    "    :param dim_feedforward: The size of the feedforward layer in the TextCell.\n",
    "    :param dropout_prob: The dropout probability in the TextCell.\n",
    "    :param gram_heads: The number of heads for the GramEncoder.\n",
    "    :return: An instance of BGCCell configured for text data.\n",
    "    \"\"\"\n",
    "    # Build encoder\n",
    "    encoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=num_heads,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=dim_feedforward\n",
    "    )\n",
    "    encoder_cell = BGCEncoderCell(encoder, gram_encoder, feedforward)\n",
    "\n",
    "    # Build decoder\n",
    "    decoder = TextCell(\n",
    "        num_layers=num_layers,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=num_heads,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    decoder_cell = BGCDecoderCell(decoder)\n",
    "\n",
    "    # Return BGCCell\n",
    "    return BGCCell(encoder_cell, decoder_cell)\n",
    "\n",
    "def build_bgc_cell_for_image(\n",
    "        num_layers: int,\n",
    "        embedding_dim: int,\n",
    "        kernel_size: int,\n",
    "        dropout_prob: float,\n",
    "        gram_heads: int,\n",
    "        **kwargs\n",
    "    ) -> BGCCell:\n",
    "    \"\"\"\n",
    "    Build a BGCCell for image data.\n",
    "    :param num_layers: Number of layers in the PixelCell.\n",
    "    :param embedding_dim: The dimension of the embeddings.\n",
    "    :param kernel_size: The size of the convolutional kernel in the PixelCell.\n",
    "    :param dropout_prob: The dropout probability in the PixelCell.\n",
    "    :param gram_heads: The number of heads for the GramEncoder.\n",
    "    :return: An instance of BGCCell configured for image data.\n",
    "    \"\"\"\n",
    "    # Build encoder\n",
    "    encoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    gram_encoder = GramEncoder(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_heads=gram_heads\n",
    "    )\n",
    "    feedforward = FeedForward(\n",
    "        embedding_dim=embedding_dim,\n",
    "        feedforward_dim=dim_feedforward\n",
    "    )\n",
    "    encoder_cell = BGCEncoderCell(encoder, gram_encoder, feedforward)\n",
    "\n",
    "    # Build decoder\n",
    "    decoder = PixelCell(\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_layers=num_layers,\n",
    "        kernel_size=kernel_size,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    decoder_cell = BGCDecoderCell(decoder)\n",
    "\n",
    "    # Return BGCCell\n",
    "    return BGCCell(encoder_cell, decoder_cell)"
   ],
   "id": "366aef0a41d44add"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## BidirectionalGramConverter\n",
    "\n",
    "The main converter model. It can convert into latent space, or from latent space back into an image grid. It has an encode mode, and a decode mode. \n",
    "\n",
    "**Premise**\n",
    "\n",
    "* We can use the fact addition and subtraction are inverses to encode and decode respectively\n",
    "* gram encodings make good latent representations for this task\n",
    "\n",
    "**Dependencies**\n",
    "\n",
    "*primary*\n",
    "\n",
    "* embeddings: a nn.Embedding layer. Will be used to embed the int grid to a set of embeddings.\n",
    "* logits: a nn.Module. Converts an embedding to logits that can be interpreted. \n",
    "* cells List[BGCCell]: A sequence of BGCCells which are the actual actions that will be done\n",
    "* pos_encoding_2d: A positional encoding 2d mechanism that provides encodings with dimensionality embed_dim/2\n",
    "\n",
    "*config*\n",
    "\n",
    "* return_internal_embeddings [Bool, default: false]: Return the intermediate embeddings in a tensor\n",
    "\n",
    "### Method: compute_pos_encoding\n",
    "\n",
    "A helper method. Used to precompute positional encodings that contain positional information and the\n",
    "information about the shape of the grid integrated\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* mask: The mask for the case. (batch x N x M)\n",
    "\n",
    "**Returns** \n",
    "\n",
    "* pos_encoding_2d: A specialized 2d positional encoding used to encode x position, y position, and grid shape. (batch x N x M)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We operate as follows.\n",
    "\n",
    "* Figure out the size of the grid based on the mask. We get an int for x, and an int for y\n",
    "* create 2d positional encodings big enough for the input based on the precomputed grid. This will only be half as long as it needs to be.\n",
    "* Extract from the precomputed grid the element at x_size, y_size. Concatenate it onto the pos encodings we are building.\n",
    "* return the encodings\n",
    "\n",
    "This will mean part of the encoding encodes grid position, and another portion encodes grid size.\n",
    "\n",
    "### Method: encode\n",
    "\n",
    "Encodes the latent representation as a sequence of gram embeddings\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* input: A grid of integers. (batch x N x M)\n",
    "* mask: A mask indicating what items are active. (batch x M x N)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "If not return_internal_embeddings:\n",
    "\n",
    "* embeddings: The output embeddings after processing (batch x N x M x E)\n",
    "* gram_embeddings: The stack of gram embeddings (batch x L x E)\n",
    "\n",
    "Else:\n",
    "\n",
    "* embeddings: The output embeddings after processing. (batch x N x M x E)\n",
    "* gram_embeddings: The stack of gram embeddings. (batch x L x E)\n",
    "\n",
    "* internal_embeddings: The embeddings produced at each intermediate layer. (batch x L x N x M x E)\n",
    "\n",
    "**Design**\n",
    "\n",
    "We embed, encode positional encodings, and then run the encoding methods in order\n",
    "\n",
    "* Embed: Run the input through the embedding layer. Mask it.\n",
    "* Build encodings: Build the positional encodings\n",
    "* Run layers: For each BGCCEll, moving forward\n",
    "    * Call .encode\n",
    "    * Store resulting gram embedding\n",
    "    * update current embedding\n",
    "    * if relevant, store intermediate embedding\n",
    "* Stack gram_embeddings, if relevant stack intermediate embeddings\n",
    "* return contents\n",
    "\n",
    "### Method: decode\n",
    "\n",
    "Decodes a latent representation based on gram embeddings back into the original content. Does this by running the layers in reverse\n",
    "\n",
    "**Accepts**\n",
    "\n",
    "* embeddings: A grid of pixel embeddings. (batch x N x M x E). May be filled with zeros\n",
    "* mask: A activity mask. (batch x N x M). True means include\n",
    "* gram_embeddings: The gram embeddings. (batch x L x E)\n",
    "\n",
    "**Returns**\n",
    "\n",
    "If not return_internal_embeddings:\n",
    "\n",
    "* embeddings: The decoded embeddings\n",
    "* logits: A logit projection for each encoding.\n",
    "\n",
    "Else:\n",
    "   \n",
    "* embeddings: The decoded embeddings\n",
    "* logits: A logit projection for each encoding.\n",
    "* internal_embeddings: embedding stack from the various layers. Drawn from the same place as in the input encoding process.\n",
    "\n",
    "**Design**\n",
    "\n",
    "We basically run the encode process but in reverse. \n",
    "\n",
    "Start from the embedding and the gram_embeddings. For each gram_embedding, embedding running in reverse, call the associated decode method. Then project the embedding into logits and return."
   ],
   "id": "cb376a1241f0c9cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
